{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.12 Kaggle上的图像分类（CIFAR-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.2.0\n"
     ]
    }
   ],
   "source": [
    "# 本节的网络需要较长的训练时间\n",
    "# 可以在Kaggle访问：\n",
    "# https://www.kaggle.com/boyuai/boyu-d2l-image-classification-cifar-10\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取和组织数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比赛数据分为训练集和测试集。训练集包含 50,000 图片。测试集包含 300,000 图片。两个数据集中的图像格式均为PNG，高度和宽度均为32像素，并具有三个颜色通道（RGB）。图像涵盖10个类别：飞机，汽车，鸟类，猫，鹿，狗，青蛙，马，船和卡车。 为了更容易上手，我们提供了上述数据集的小样本。“ train_tiny.zip”包含 80 训练样本，而“ test_tiny.zip”包含100个测试样本。它们的未压缩文件夹名称分别是“ train_tiny”和“ test_tiny”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图像增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图像增强\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),  #先四周填充0，再把图像随机裁剪成32*32\n",
    "    transforms.RandomHorizontalFlip(),  #图像一半的概率翻转，一半的概率不翻转\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4731, 0.4822, 0.4465), (0.2212, 0.1994, 0.2010)), #R,G,B每层的归一化用到的均值和方差\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4731, 0.4822, 0.4465), (0.2212, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=torchvision.datasets.CIFAR10(train=True, root=\"~/Datasets/CIFAR\", transform=transform_train,download=False)\n",
    "trainloader = torch.utils.data.DataLoader(train, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=torchvision.datasets.CIFAR10(train=False, root=\"~/Datasets/CIFAR\", transform=transform_test, download=False)\n",
    "testloader = torch.utils.data.DataLoader(test, batch_size=256, shuffle=False)\n",
    "\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'forg', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet-18网络结构：ResNet全名Residual Network残差网络。Kaiming He 的《Deep Residual Learning for Image Recognition》获得了CVPR最佳论文。他提出的深度残差网络在2015年可以说是洗刷了图像方面的各大比赛，以绝对优势取得了多个比赛的冠军。而且它在保证网络精度的前提下，将网络的深度达到了152层，后来又进一步加到1000的深度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):   # 我们定义网络时一般是继承的torch.nn.Module创建新的子类\n",
    "\n",
    "    def __init__(self, inchannel, outchannel, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        #torch.nn.Sequential是一个Sequential容器，模块将按照构造函数中传递的顺序添加到模块中。\n",
    "        self.left = nn.Sequential(\n",
    "            nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False), \n",
    "            # 添加第一个卷积层,调用了nn里面的Conv2d（）\n",
    "            nn.BatchNorm2d(outchannel), # 进行数据的归一化处理\n",
    "            nn.ReLU(inplace=True), # 修正线性单元，是一种人工神经网络中常用的激活函数\n",
    "            nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel)\n",
    "        )\n",
    "        self.shortcut = nn.Sequential() \n",
    "        if stride != 1 or inchannel != outchannel:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(outchannel)\n",
    "            )\n",
    "        #  便于之后的联合,要判断Y = self.left(X)的形状是否与X相同\n",
    "\n",
    "    def forward(self, x): # 将两个模块的特征进行结合，并使用ReLU激活函数得到最终的特征。\n",
    "        out = self.left(x)\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, ResidualBlock, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inchannel = 64\n",
    "        self.conv1 = nn.Sequential( # 用3个3x3的卷积核代替7x7的卷积核，减少模型参数\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "        ) \n",
    "        self.layer1 = self.make_layer(ResidualBlock, 64,  2, stride=1)\n",
    "        self.layer2 = self.make_layer(ResidualBlock, 128, 2, stride=2)\n",
    "        self.layer3 = self.make_layer(ResidualBlock, 256, 2, stride=2)\n",
    "        self.layer4 = self.make_layer(ResidualBlock, 512, 2, stride=2)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def make_layer(self, block, channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)   #第一个ResidualBlock的步幅由make_layer的函数参数stride指定\n",
    "        # ，后续的num_blocks-1个ResidualBlock步幅是1\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.inchannel, channels, stride))\n",
    "            self.inchannel = channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(ResidualBlock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练和测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training, Resnet-18!\n",
      "\n",
      "Epoch: 1\n",
      "[epoch:1, iter:20] Loss: 831.080 | Acc: 12.402% \n",
      "[epoch:1, iter:40] Loss: 709.201 | Acc: 15.752% \n",
      "[epoch:1, iter:60] Loss: 650.907 | Acc: 18.958% \n",
      "[epoch:1, iter:80] Loss: 609.761 | Acc: 21.484% \n",
      "[epoch:1, iter:100] Loss: 581.609 | Acc: 23.352% \n",
      "[epoch:1, iter:120] Loss: 560.863 | Acc: 25.085% \n",
      "[epoch:1, iter:140] Loss: 542.538 | Acc: 26.613% \n",
      "[epoch:1, iter:160] Loss: 528.997 | Acc: 27.881% \n",
      "[epoch:1, iter:180] Loss: 518.231 | Acc: 28.895% \n",
      "\n",
      "Epoch: 2\n",
      "[epoch:2, iter:216] Loss: 409.667 | Acc: 39.902% \n",
      "[epoch:2, iter:236] Loss: 404.361 | Acc: 41.162% \n",
      "[epoch:2, iter:256] Loss: 399.515 | Acc: 42.207% \n",
      "[epoch:2, iter:276] Loss: 397.477 | Acc: 42.632% \n",
      "[epoch:2, iter:296] Loss: 396.152 | Acc: 42.828% \n",
      "[epoch:2, iter:316] Loss: 394.245 | Acc: 43.066% \n",
      "[epoch:2, iter:336] Loss: 391.550 | Acc: 43.477% \n",
      "[epoch:2, iter:356] Loss: 387.760 | Acc: 44.053% \n",
      "[epoch:2, iter:376] Loss: 384.452 | Acc: 44.559% \n",
      "\n",
      "Epoch: 3\n",
      "[epoch:3, iter:412] Loss: 356.008 | Acc: 48.691% \n",
      "[epoch:3, iter:432] Loss: 349.245 | Acc: 49.824% \n",
      "[epoch:3, iter:452] Loss: 343.773 | Acc: 51.022% \n",
      "[epoch:3, iter:472] Loss: 340.746 | Acc: 51.436% \n",
      "[epoch:3, iter:492] Loss: 337.210 | Acc: 51.984% \n",
      "[epoch:3, iter:512] Loss: 335.298 | Acc: 52.174% \n",
      "[epoch:3, iter:532] Loss: 331.454 | Acc: 52.846% \n",
      "[epoch:3, iter:552] Loss: 328.147 | Acc: 53.367% \n",
      "[epoch:3, iter:572] Loss: 326.003 | Acc: 53.711% \n",
      "\n",
      "Epoch: 4\n",
      "[epoch:4, iter:608] Loss: 297.574 | Acc: 57.617% \n",
      "[epoch:4, iter:628] Loss: 297.149 | Acc: 57.998% \n",
      "[epoch:4, iter:648] Loss: 291.659 | Acc: 58.906% \n",
      "[epoch:4, iter:668] Loss: 288.106 | Acc: 59.556% \n",
      "[epoch:4, iter:688] Loss: 285.197 | Acc: 60.043% \n",
      "[epoch:4, iter:708] Loss: 282.024 | Acc: 60.570% \n",
      "[epoch:4, iter:728] Loss: 278.915 | Acc: 60.968% \n",
      "[epoch:4, iter:748] Loss: 277.449 | Acc: 61.284% \n",
      "[epoch:4, iter:768] Loss: 274.487 | Acc: 61.753% \n",
      "\n",
      "Epoch: 5\n",
      "[epoch:5, iter:804] Loss: 243.394 | Acc: 65.566% \n",
      "[epoch:5, iter:824] Loss: 244.395 | Acc: 65.645% \n",
      "[epoch:5, iter:844] Loss: 240.986 | Acc: 66.152% \n",
      "[epoch:5, iter:864] Loss: 239.944 | Acc: 66.372% \n",
      "[epoch:5, iter:884] Loss: 238.024 | Acc: 66.695% \n",
      "[epoch:5, iter:904] Loss: 235.633 | Acc: 66.999% \n",
      "[epoch:5, iter:924] Loss: 235.024 | Acc: 67.190% \n",
      "[epoch:5, iter:944] Loss: 233.690 | Acc: 67.422% \n",
      "[epoch:5, iter:964] Loss: 232.190 | Acc: 67.585% \n",
      "\n",
      "Epoch: 6\n",
      "[epoch:6, iter:1000] Loss: 213.795 | Acc: 69.590% \n",
      "[epoch:6, iter:1020] Loss: 212.112 | Acc: 70.078% \n",
      "[epoch:6, iter:1040] Loss: 210.370 | Acc: 70.423% \n",
      "[epoch:6, iter:1060] Loss: 208.004 | Acc: 70.864% \n",
      "[epoch:6, iter:1080] Loss: 208.304 | Acc: 71.035% \n",
      "[epoch:6, iter:1100] Loss: 207.092 | Acc: 71.296% \n",
      "[epoch:6, iter:1120] Loss: 206.801 | Acc: 71.401% \n",
      "[epoch:6, iter:1140] Loss: 204.493 | Acc: 71.760% \n",
      "[epoch:6, iter:1160] Loss: 202.940 | Acc: 71.994% \n",
      "\n",
      "Epoch: 7\n",
      "[epoch:7, iter:1196] Loss: 186.579 | Acc: 74.434% \n",
      "[epoch:7, iter:1216] Loss: 183.419 | Acc: 74.678% \n",
      "[epoch:7, iter:1236] Loss: 183.882 | Acc: 74.753% \n",
      "[epoch:7, iter:1256] Loss: 182.164 | Acc: 75.059% \n",
      "[epoch:7, iter:1276] Loss: 183.162 | Acc: 74.883% \n",
      "[epoch:7, iter:1296] Loss: 181.966 | Acc: 75.036% \n",
      "[epoch:7, iter:1316] Loss: 179.548 | Acc: 75.399% \n",
      "[epoch:7, iter:1336] Loss: 179.124 | Acc: 75.349% \n",
      "[epoch:7, iter:1356] Loss: 177.921 | Acc: 75.523% \n",
      "\n",
      "Epoch: 8\n",
      "[epoch:8, iter:1392] Loss: 159.650 | Acc: 77.773% \n",
      "[epoch:8, iter:1412] Loss: 162.357 | Acc: 77.891% \n",
      "[epoch:8, iter:1432] Loss: 163.971 | Acc: 77.708% \n",
      "[epoch:8, iter:1452] Loss: 163.120 | Acc: 77.842% \n",
      "[epoch:8, iter:1472] Loss: 162.111 | Acc: 77.953% \n",
      "[epoch:8, iter:1492] Loss: 161.696 | Acc: 78.076% \n",
      "[epoch:8, iter:1512] Loss: 161.320 | Acc: 78.128% \n",
      "[epoch:8, iter:1532] Loss: 159.992 | Acc: 78.279% \n",
      "[epoch:8, iter:1552] Loss: 159.094 | Acc: 78.370% \n",
      "\n",
      "Epoch: 9\n",
      "[epoch:9, iter:1588] Loss: 143.199 | Acc: 80.586% \n",
      "[epoch:9, iter:1608] Loss: 142.932 | Acc: 80.566% \n",
      "[epoch:9, iter:1628] Loss: 143.688 | Acc: 80.469% \n",
      "[epoch:9, iter:1648] Loss: 145.060 | Acc: 80.288% \n",
      "[epoch:9, iter:1668] Loss: 144.968 | Acc: 80.379% \n",
      "[epoch:9, iter:1688] Loss: 145.297 | Acc: 80.368% \n",
      "[epoch:9, iter:1708] Loss: 145.064 | Acc: 80.499% \n",
      "[epoch:9, iter:1728] Loss: 144.825 | Acc: 80.571% \n",
      "[epoch:9, iter:1748] Loss: 143.544 | Acc: 80.699% \n",
      "\n",
      "Epoch: 10\n",
      "[epoch:10, iter:1784] Loss: 143.706 | Acc: 80.527% \n",
      "[epoch:10, iter:1804] Loss: 139.777 | Acc: 80.781% \n",
      "[epoch:10, iter:1824] Loss: 138.536 | Acc: 80.957% \n",
      "[epoch:10, iter:1844] Loss: 137.688 | Acc: 81.099% \n",
      "[epoch:10, iter:1864] Loss: 138.123 | Acc: 81.031% \n",
      "[epoch:10, iter:1884] Loss: 137.187 | Acc: 81.172% \n",
      "[epoch:10, iter:1904] Loss: 135.904 | Acc: 81.406% \n",
      "[epoch:10, iter:1924] Loss: 135.664 | Acc: 81.545% \n",
      "[epoch:10, iter:1944] Loss: 134.527 | Acc: 81.758% \n",
      "\n",
      "Epoch: 11\n",
      "[epoch:11, iter:1980] Loss: 119.296 | Acc: 83.906% \n",
      "[epoch:11, iter:2000] Loss: 120.704 | Acc: 83.721% \n",
      "[epoch:11, iter:2020] Loss: 122.902 | Acc: 83.255% \n",
      "[epoch:11, iter:2040] Loss: 124.920 | Acc: 83.101% \n",
      "[epoch:11, iter:2060] Loss: 124.939 | Acc: 83.070% \n",
      "[epoch:11, iter:2080] Loss: 124.171 | Acc: 83.203% \n",
      "[epoch:11, iter:2100] Loss: 123.959 | Acc: 83.147% \n",
      "[epoch:11, iter:2120] Loss: 122.355 | Acc: 83.401% \n",
      "[epoch:11, iter:2140] Loss: 121.752 | Acc: 83.433% \n",
      "\n",
      "Epoch: 12\n",
      "[epoch:12, iter:2176] Loss: 116.428 | Acc: 84.297% \n",
      "[epoch:12, iter:2196] Loss: 116.795 | Acc: 84.219% \n",
      "[epoch:12, iter:2216] Loss: 115.216 | Acc: 84.440% \n",
      "[epoch:12, iter:2236] Loss: 114.197 | Acc: 84.487% \n",
      "[epoch:12, iter:2256] Loss: 115.029 | Acc: 84.402% \n",
      "[epoch:12, iter:2276] Loss: 114.500 | Acc: 84.463% \n",
      "[epoch:12, iter:2296] Loss: 114.965 | Acc: 84.475% \n",
      "[epoch:12, iter:2316] Loss: 114.759 | Acc: 84.441% \n",
      "[epoch:12, iter:2336] Loss: 114.217 | Acc: 84.518% \n",
      "\n",
      "Epoch: 13\n",
      "[epoch:13, iter:2372] Loss: 112.957 | Acc: 84.863% \n",
      "[epoch:13, iter:2392] Loss: 110.870 | Acc: 84.990% \n",
      "[epoch:13, iter:2412] Loss: 110.823 | Acc: 85.007% \n",
      "[epoch:13, iter:2432] Loss: 110.355 | Acc: 85.034% \n",
      "[epoch:13, iter:2452] Loss: 109.509 | Acc: 85.219% \n",
      "[epoch:13, iter:2472] Loss: 109.107 | Acc: 85.241% \n",
      "[epoch:13, iter:2492] Loss: 108.984 | Acc: 85.254% \n",
      "[epoch:13, iter:2512] Loss: 109.645 | Acc: 85.156% \n",
      "[epoch:13, iter:2532] Loss: 109.783 | Acc: 85.152% \n",
      "\n",
      "Epoch: 14\n",
      "[epoch:14, iter:2568] Loss: 105.117 | Acc: 86.250% \n",
      "[epoch:14, iter:2588] Loss: 104.014 | Acc: 86.289% \n",
      "[epoch:14, iter:2608] Loss: 103.096 | Acc: 86.380% \n",
      "[epoch:14, iter:2628] Loss: 103.086 | Acc: 86.338% \n",
      "[epoch:14, iter:2648] Loss: 102.425 | Acc: 86.441% \n",
      "[epoch:14, iter:2668] Loss: 102.434 | Acc: 86.344% \n",
      "[epoch:14, iter:2688] Loss: 102.606 | Acc: 86.345% \n",
      "[epoch:14, iter:2708] Loss: 102.262 | Acc: 86.394% \n",
      "[epoch:14, iter:2728] Loss: 102.865 | Acc: 86.233% \n",
      "\n",
      "Epoch: 15\n",
      "[epoch:15, iter:2764] Loss: 95.553 | Acc: 86.758% \n",
      "[epoch:15, iter:2784] Loss: 94.875 | Acc: 86.885% \n",
      "[epoch:15, iter:2804] Loss: 94.579 | Acc: 86.966% \n",
      "[epoch:15, iter:2824] Loss: 95.435 | Acc: 86.938% \n",
      "[epoch:15, iter:2844] Loss: 95.568 | Acc: 86.828% \n",
      "[epoch:15, iter:2864] Loss: 96.513 | Acc: 86.781% \n",
      "[epoch:15, iter:2884] Loss: 96.227 | Acc: 86.881% \n",
      "[epoch:15, iter:2904] Loss: 96.934 | Acc: 86.838% \n",
      "[epoch:15, iter:2924] Loss: 97.691 | Acc: 86.721% \n",
      "\n",
      "Epoch: 16\n",
      "[epoch:16, iter:2960] Loss: 98.352 | Acc: 86.230% \n",
      "[epoch:16, iter:2980] Loss: 92.365 | Acc: 87.441% \n",
      "[epoch:16, iter:3000] Loss: 91.215 | Acc: 87.493% \n",
      "[epoch:16, iter:3020] Loss: 92.231 | Acc: 87.466% \n",
      "[epoch:16, iter:3040] Loss: 92.500 | Acc: 87.527% \n",
      "[epoch:16, iter:3060] Loss: 92.128 | Acc: 87.598% \n",
      "[epoch:16, iter:3080] Loss: 91.490 | Acc: 87.695% \n",
      "[epoch:16, iter:3100] Loss: 91.934 | Acc: 87.627% \n",
      "[epoch:16, iter:3120] Loss: 93.224 | Acc: 87.424% \n",
      "\n",
      "Epoch: 17\n",
      "[epoch:17, iter:3156] Loss: 94.804 | Acc: 87.148% \n",
      "[epoch:17, iter:3176] Loss: 90.552 | Acc: 87.725% \n",
      "[epoch:17, iter:3196] Loss: 88.827 | Acc: 87.878% \n",
      "[epoch:17, iter:3216] Loss: 88.507 | Acc: 87.866% \n",
      "[epoch:17, iter:3236] Loss: 87.615 | Acc: 88.082% \n",
      "[epoch:17, iter:3256] Loss: 87.980 | Acc: 87.995% \n",
      "[epoch:17, iter:3276] Loss: 88.862 | Acc: 87.916% \n",
      "[epoch:17, iter:3296] Loss: 89.110 | Acc: 87.920% \n",
      "[epoch:17, iter:3316] Loss: 88.878 | Acc: 87.986% \n",
      "\n",
      "Epoch: 18\n",
      "[epoch:18, iter:3352] Loss: 80.298 | Acc: 89.551% \n",
      "[epoch:18, iter:3372] Loss: 83.552 | Acc: 88.926% \n",
      "[epoch:18, iter:3392] Loss: 85.685 | Acc: 88.438% \n",
      "[epoch:18, iter:3412] Loss: 85.528 | Acc: 88.442% \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:18, iter:3432] Loss: 85.270 | Acc: 88.480% \n",
      "[epoch:18, iter:3452] Loss: 85.785 | Acc: 88.421% \n",
      "[epoch:18, iter:3472] Loss: 86.034 | Acc: 88.351% \n",
      "[epoch:18, iter:3492] Loss: 85.883 | Acc: 88.406% \n",
      "[epoch:18, iter:3512] Loss: 86.137 | Acc: 88.407% \n",
      "\n",
      "Epoch: 19\n",
      "[epoch:19, iter:3548] Loss: 83.441 | Acc: 88.867% \n",
      "[epoch:19, iter:3568] Loss: 83.848 | Acc: 88.662% \n",
      "[epoch:19, iter:3588] Loss: 84.921 | Acc: 88.587% \n",
      "[epoch:19, iter:3608] Loss: 84.063 | Acc: 88.716% \n",
      "[epoch:19, iter:3628] Loss: 84.144 | Acc: 88.738% \n",
      "[epoch:19, iter:3648] Loss: 84.332 | Acc: 88.737% \n",
      "[epoch:19, iter:3668] Loss: 85.021 | Acc: 88.624% \n",
      "[epoch:19, iter:3688] Loss: 84.926 | Acc: 88.638% \n",
      "[epoch:19, iter:3708] Loss: 85.135 | Acc: 88.620% \n",
      "\n",
      "Epoch: 20\n",
      "[epoch:20, iter:3744] Loss: 83.721 | Acc: 88.691% \n",
      "[epoch:20, iter:3764] Loss: 82.130 | Acc: 88.916% \n",
      "[epoch:20, iter:3784] Loss: 82.020 | Acc: 88.991% \n",
      "[epoch:20, iter:3804] Loss: 81.023 | Acc: 89.146% \n",
      "[epoch:20, iter:3824] Loss: 81.267 | Acc: 89.000% \n",
      "[epoch:20, iter:3844] Loss: 80.683 | Acc: 89.072% \n",
      "[epoch:20, iter:3864] Loss: 80.861 | Acc: 89.088% \n",
      "[epoch:20, iter:3884] Loss: 81.765 | Acc: 88.950% \n",
      "[epoch:20, iter:3904] Loss: 81.655 | Acc: 88.971% \n",
      "\n",
      "Epoch: 21\n",
      "[epoch:21, iter:3940] Loss: 81.119 | Acc: 89.102% \n",
      "[epoch:21, iter:3960] Loss: 81.889 | Acc: 89.004% \n",
      "[epoch:21, iter:3980] Loss: 81.573 | Acc: 89.056% \n",
      "[epoch:21, iter:4000] Loss: 80.742 | Acc: 89.199% \n",
      "[epoch:21, iter:4020] Loss: 80.562 | Acc: 89.125% \n",
      "[epoch:21, iter:4040] Loss: 81.381 | Acc: 89.049% \n",
      "[epoch:21, iter:4060] Loss: 81.431 | Acc: 88.998% \n",
      "[epoch:21, iter:4080] Loss: 81.092 | Acc: 89.060% \n",
      "[epoch:21, iter:4100] Loss: 80.664 | Acc: 89.123% \n",
      "\n",
      "Epoch: 22\n",
      "[epoch:22, iter:4136] Loss: 75.421 | Acc: 89.766% \n",
      "[epoch:22, iter:4156] Loss: 77.998 | Acc: 89.492% \n",
      "[epoch:22, iter:4176] Loss: 77.638 | Acc: 89.479% \n",
      "[epoch:22, iter:4196] Loss: 78.110 | Acc: 89.399% \n",
      "[epoch:22, iter:4216] Loss: 77.589 | Acc: 89.531% \n",
      "[epoch:22, iter:4236] Loss: 78.017 | Acc: 89.443% \n",
      "[epoch:22, iter:4256] Loss: 78.455 | Acc: 89.358% \n",
      "[epoch:22, iter:4276] Loss: 78.747 | Acc: 89.280% \n",
      "[epoch:22, iter:4296] Loss: 79.061 | Acc: 89.251% \n",
      "\n",
      "Epoch: 23\n",
      "[epoch:23, iter:4332] Loss: 75.239 | Acc: 90.410% \n",
      "[epoch:23, iter:4352] Loss: 75.002 | Acc: 90.293% \n",
      "[epoch:23, iter:4372] Loss: 77.860 | Acc: 89.915% \n",
      "[epoch:23, iter:4392] Loss: 76.226 | Acc: 89.961% \n",
      "[epoch:23, iter:4412] Loss: 75.361 | Acc: 90.055% \n",
      "[epoch:23, iter:4432] Loss: 75.229 | Acc: 90.085% \n",
      "[epoch:23, iter:4452] Loss: 76.113 | Acc: 89.941% \n",
      "[epoch:23, iter:4472] Loss: 76.048 | Acc: 89.919% \n",
      "[epoch:23, iter:4492] Loss: 76.498 | Acc: 89.833% \n",
      "\n",
      "Epoch: 24\n",
      "[epoch:24, iter:4528] Loss: 75.215 | Acc: 89.258% \n",
      "[epoch:24, iter:4548] Loss: 79.035 | Acc: 89.131% \n",
      "[epoch:24, iter:4568] Loss: 78.280 | Acc: 89.271% \n",
      "[epoch:24, iter:4588] Loss: 77.259 | Acc: 89.463% \n",
      "[epoch:24, iter:4608] Loss: 75.898 | Acc: 89.688% \n",
      "[epoch:24, iter:4628] Loss: 75.419 | Acc: 89.749% \n",
      "[epoch:24, iter:4648] Loss: 75.851 | Acc: 89.682% \n",
      "[epoch:24, iter:4668] Loss: 76.226 | Acc: 89.609% \n",
      "[epoch:24, iter:4688] Loss: 75.957 | Acc: 89.661% \n",
      "\n",
      "Epoch: 25\n",
      "[epoch:25, iter:4724] Loss: 67.180 | Acc: 91.035% \n",
      "[epoch:25, iter:4744] Loss: 69.954 | Acc: 90.527% \n",
      "[epoch:25, iter:4764] Loss: 69.056 | Acc: 90.703% \n",
      "[epoch:25, iter:4784] Loss: 69.096 | Acc: 90.679% \n",
      "[epoch:25, iter:4804] Loss: 70.019 | Acc: 90.559% \n",
      "[epoch:25, iter:4824] Loss: 70.947 | Acc: 90.443% \n",
      "[epoch:25, iter:4844] Loss: 71.843 | Acc: 90.287% \n",
      "[epoch:25, iter:4864] Loss: 72.087 | Acc: 90.310% \n",
      "[epoch:25, iter:4884] Loss: 72.036 | Acc: 90.310% \n",
      "\n",
      "Epoch: 26\n",
      "[epoch:26, iter:4920] Loss: 72.906 | Acc: 90.020% \n",
      "[epoch:26, iter:4940] Loss: 72.873 | Acc: 90.195% \n",
      "[epoch:26, iter:4960] Loss: 73.003 | Acc: 90.195% \n",
      "[epoch:26, iter:4980] Loss: 72.362 | Acc: 90.225% \n",
      "[epoch:26, iter:5000] Loss: 72.652 | Acc: 90.203% \n",
      "[epoch:26, iter:5020] Loss: 72.696 | Acc: 90.238% \n",
      "[epoch:26, iter:5040] Loss: 71.967 | Acc: 90.357% \n",
      "[epoch:26, iter:5060] Loss: 71.805 | Acc: 90.405% \n",
      "[epoch:26, iter:5080] Loss: 71.803 | Acc: 90.404% \n",
      "\n",
      "Epoch: 27\n",
      "[epoch:27, iter:5116] Loss: 67.950 | Acc: 90.449% \n",
      "[epoch:27, iter:5136] Loss: 67.487 | Acc: 90.654% \n",
      "[epoch:27, iter:5156] Loss: 69.506 | Acc: 90.469% \n",
      "[epoch:27, iter:5176] Loss: 69.532 | Acc: 90.483% \n",
      "[epoch:27, iter:5196] Loss: 68.627 | Acc: 90.625% \n",
      "[epoch:27, iter:5216] Loss: 68.782 | Acc: 90.645% \n",
      "[epoch:27, iter:5236] Loss: 68.645 | Acc: 90.656% \n",
      "[epoch:27, iter:5256] Loss: 69.573 | Acc: 90.544% \n",
      "[epoch:27, iter:5276] Loss: 70.083 | Acc: 90.460% \n",
      "\n",
      "Epoch: 28\n",
      "[epoch:28, iter:5312] Loss: 64.205 | Acc: 91.113% \n",
      "[epoch:28, iter:5332] Loss: 65.632 | Acc: 90.986% \n",
      "[epoch:28, iter:5352] Loss: 65.697 | Acc: 91.009% \n",
      "[epoch:28, iter:5372] Loss: 67.281 | Acc: 90.791% \n",
      "[epoch:28, iter:5392] Loss: 68.217 | Acc: 90.754% \n",
      "[epoch:28, iter:5412] Loss: 69.405 | Acc: 90.579% \n",
      "[epoch:28, iter:5432] Loss: 69.112 | Acc: 90.684% \n",
      "[epoch:28, iter:5452] Loss: 69.094 | Acc: 90.681% \n",
      "[epoch:28, iter:5472] Loss: 69.459 | Acc: 90.614% \n",
      "\n",
      "Epoch: 29\n",
      "[epoch:29, iter:5508] Loss: 66.754 | Acc: 90.801% \n",
      "[epoch:29, iter:5528] Loss: 68.630 | Acc: 90.557% \n",
      "[epoch:29, iter:5548] Loss: 68.779 | Acc: 90.560% \n",
      "[epoch:29, iter:5568] Loss: 68.524 | Acc: 90.591% \n",
      "[epoch:29, iter:5588] Loss: 68.092 | Acc: 90.672% \n",
      "[epoch:29, iter:5608] Loss: 68.369 | Acc: 90.661% \n",
      "[epoch:29, iter:5628] Loss: 67.681 | Acc: 90.737% \n",
      "[epoch:29, iter:5648] Loss: 68.087 | Acc: 90.742% \n",
      "[epoch:29, iter:5668] Loss: 68.211 | Acc: 90.751% \n",
      "\n",
      "Epoch: 30\n",
      "[epoch:30, iter:5704] Loss: 67.978 | Acc: 91.113% \n",
      "[epoch:30, iter:5724] Loss: 65.394 | Acc: 91.445% \n",
      "[epoch:30, iter:5744] Loss: 64.510 | Acc: 91.367% \n",
      "[epoch:30, iter:5764] Loss: 65.157 | Acc: 91.343% \n",
      "[epoch:30, iter:5784] Loss: 65.318 | Acc: 91.234% \n",
      "[epoch:30, iter:5804] Loss: 66.459 | Acc: 91.003% \n",
      "[epoch:30, iter:5824] Loss: 66.357 | Acc: 91.018% \n",
      "[epoch:30, iter:5844] Loss: 66.409 | Acc: 91.067% \n",
      "[epoch:30, iter:5864] Loss: 66.808 | Acc: 91.009% \n",
      "\n",
      "Epoch: 31\n",
      "[epoch:31, iter:5900] Loss: 68.291 | Acc: 90.703% \n",
      "[epoch:31, iter:5920] Loss: 66.149 | Acc: 91.006% \n",
      "[epoch:31, iter:5940] Loss: 65.410 | Acc: 91.100% \n",
      "[epoch:31, iter:5960] Loss: 65.800 | Acc: 91.021% \n",
      "[epoch:31, iter:5980] Loss: 64.996 | Acc: 91.121% \n",
      "[epoch:31, iter:6000] Loss: 64.079 | Acc: 91.250% \n",
      "[epoch:31, iter:6020] Loss: 64.766 | Acc: 91.194% \n",
      "[epoch:31, iter:6040] Loss: 65.315 | Acc: 91.167% \n",
      "[epoch:31, iter:6060] Loss: 65.280 | Acc: 91.128% \n",
      "\n",
      "Epoch: 32\n",
      "[epoch:32, iter:6096] Loss: 65.741 | Acc: 90.801% \n",
      "[epoch:32, iter:6116] Loss: 62.371 | Acc: 91.455% \n",
      "[epoch:32, iter:6136] Loss: 64.113 | Acc: 91.178% \n",
      "[epoch:32, iter:6156] Loss: 62.866 | Acc: 91.411% \n",
      "[epoch:32, iter:6176] Loss: 62.484 | Acc: 91.441% \n",
      "[epoch:32, iter:6196] Loss: 62.805 | Acc: 91.396% \n",
      "[epoch:32, iter:6216] Loss: 63.434 | Acc: 91.403% \n",
      "[epoch:32, iter:6236] Loss: 63.568 | Acc: 91.436% \n",
      "[epoch:32, iter:6256] Loss: 63.597 | Acc: 91.463% \n",
      "\n",
      "Epoch: 33\n",
      "[epoch:33, iter:6292] Loss: 66.526 | Acc: 91.172% \n",
      "[epoch:33, iter:6312] Loss: 63.970 | Acc: 91.475% \n",
      "[epoch:33, iter:6332] Loss: 62.430 | Acc: 91.667% \n",
      "[epoch:33, iter:6352] Loss: 62.199 | Acc: 91.704% \n",
      "[epoch:33, iter:6372] Loss: 61.475 | Acc: 91.824% \n",
      "[epoch:33, iter:6392] Loss: 60.381 | Acc: 92.057% \n",
      "[epoch:33, iter:6412] Loss: 61.808 | Acc: 91.825% \n",
      "[epoch:33, iter:6432] Loss: 62.230 | Acc: 91.733% \n",
      "[epoch:33, iter:6452] Loss: 61.869 | Acc: 91.751% \n",
      "\n",
      "Epoch: 34\n",
      "[epoch:34, iter:6488] Loss: 62.407 | Acc: 91.602% \n",
      "[epoch:34, iter:6508] Loss: 63.469 | Acc: 91.426% \n",
      "[epoch:34, iter:6528] Loss: 63.064 | Acc: 91.510% \n",
      "[epoch:34, iter:6548] Loss: 64.266 | Acc: 91.289% \n",
      "[epoch:34, iter:6568] Loss: 64.949 | Acc: 91.227% \n",
      "[epoch:34, iter:6588] Loss: 65.559 | Acc: 91.100% \n",
      "[epoch:34, iter:6608] Loss: 65.751 | Acc: 91.007% \n",
      "[epoch:34, iter:6628] Loss: 66.184 | Acc: 90.984% \n",
      "[epoch:34, iter:6648] Loss: 66.366 | Acc: 90.966% \n",
      "\n",
      "Epoch: 35\n",
      "[epoch:35, iter:6684] Loss: 65.516 | Acc: 91.641% \n",
      "[epoch:35, iter:6704] Loss: 63.672 | Acc: 91.582% \n",
      "[epoch:35, iter:6724] Loss: 62.700 | Acc: 91.758% \n",
      "[epoch:35, iter:6744] Loss: 62.329 | Acc: 91.812% \n",
      "[epoch:35, iter:6764] Loss: 61.878 | Acc: 91.832% \n",
      "[epoch:35, iter:6784] Loss: 61.539 | Acc: 91.797% \n",
      "[epoch:35, iter:6804] Loss: 61.900 | Acc: 91.766% \n",
      "[epoch:35, iter:6824] Loss: 61.526 | Acc: 91.799% \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:35, iter:6844] Loss: 61.146 | Acc: 91.832% \n",
      "\n",
      "Epoch: 36\n",
      "[epoch:36, iter:6880] Loss: 58.077 | Acc: 92.051% \n",
      "[epoch:36, iter:6900] Loss: 55.701 | Acc: 92.510% \n",
      "[epoch:36, iter:6920] Loss: 56.226 | Acc: 92.370% \n",
      "[epoch:36, iter:6940] Loss: 56.078 | Acc: 92.407% \n",
      "[epoch:36, iter:6960] Loss: 58.256 | Acc: 92.113% \n",
      "[epoch:36, iter:6980] Loss: 59.475 | Acc: 91.917% \n",
      "[epoch:36, iter:7000] Loss: 60.058 | Acc: 91.875% \n",
      "[epoch:36, iter:7020] Loss: 61.178 | Acc: 91.765% \n",
      "[epoch:36, iter:7040] Loss: 61.313 | Acc: 91.762% \n",
      "\n",
      "Epoch: 37\n",
      "[epoch:37, iter:7076] Loss: 56.806 | Acc: 92.559% \n",
      "[epoch:37, iter:7096] Loss: 56.083 | Acc: 92.373% \n",
      "[epoch:37, iter:7116] Loss: 55.857 | Acc: 92.448% \n",
      "[epoch:37, iter:7136] Loss: 57.677 | Acc: 92.271% \n",
      "[epoch:37, iter:7156] Loss: 58.564 | Acc: 92.102% \n",
      "[epoch:37, iter:7176] Loss: 59.019 | Acc: 92.048% \n",
      "[epoch:37, iter:7196] Loss: 59.180 | Acc: 92.065% \n",
      "[epoch:37, iter:7216] Loss: 59.544 | Acc: 92.007% \n",
      "[epoch:37, iter:7236] Loss: 59.866 | Acc: 91.949% \n",
      "\n",
      "Epoch: 38\n",
      "[epoch:38, iter:7272] Loss: 52.641 | Acc: 92.852% \n",
      "[epoch:38, iter:7292] Loss: 54.569 | Acc: 92.598% \n",
      "[epoch:38, iter:7312] Loss: 57.842 | Acc: 92.188% \n",
      "[epoch:38, iter:7332] Loss: 58.662 | Acc: 92.188% \n",
      "[epoch:38, iter:7352] Loss: 59.369 | Acc: 91.996% \n",
      "[epoch:38, iter:7372] Loss: 58.836 | Acc: 91.986% \n",
      "[epoch:38, iter:7392] Loss: 58.681 | Acc: 92.095% \n",
      "[epoch:38, iter:7412] Loss: 58.959 | Acc: 92.043% \n",
      "[epoch:38, iter:7432] Loss: 59.506 | Acc: 91.966% \n",
      "\n",
      "Epoch: 39\n",
      "[epoch:39, iter:7468] Loss: 65.102 | Acc: 91.230% \n",
      "[epoch:39, iter:7488] Loss: 61.297 | Acc: 91.689% \n",
      "[epoch:39, iter:7508] Loss: 58.993 | Acc: 92.174% \n",
      "[epoch:39, iter:7528] Loss: 58.136 | Acc: 92.227% \n",
      "[epoch:39, iter:7548] Loss: 58.272 | Acc: 92.219% \n",
      "[epoch:39, iter:7568] Loss: 58.984 | Acc: 92.139% \n",
      "[epoch:39, iter:7588] Loss: 59.658 | Acc: 92.087% \n",
      "[epoch:39, iter:7608] Loss: 59.726 | Acc: 92.095% \n",
      "[epoch:39, iter:7628] Loss: 59.833 | Acc: 92.075% \n",
      "\n",
      "Epoch: 40\n",
      "[epoch:40, iter:7664] Loss: 59.411 | Acc: 92.266% \n",
      "[epoch:40, iter:7684] Loss: 56.243 | Acc: 92.559% \n",
      "[epoch:40, iter:7704] Loss: 55.842 | Acc: 92.520% \n",
      "[epoch:40, iter:7724] Loss: 55.803 | Acc: 92.529% \n",
      "[epoch:40, iter:7744] Loss: 57.120 | Acc: 92.340% \n",
      "[epoch:40, iter:7764] Loss: 57.327 | Acc: 92.327% \n",
      "[epoch:40, iter:7784] Loss: 57.346 | Acc: 92.307% \n",
      "[epoch:40, iter:7804] Loss: 57.638 | Acc: 92.239% \n",
      "[epoch:40, iter:7824] Loss: 57.987 | Acc: 92.153% \n",
      "\n",
      "Epoch: 41\n",
      "[epoch:41, iter:7860] Loss: 64.284 | Acc: 91.660% \n",
      "[epoch:41, iter:7880] Loss: 60.216 | Acc: 91.992% \n",
      "[epoch:41, iter:7900] Loss: 60.764 | Acc: 91.849% \n",
      "[epoch:41, iter:7920] Loss: 61.084 | Acc: 91.709% \n",
      "[epoch:41, iter:7940] Loss: 60.517 | Acc: 91.836% \n",
      "[epoch:41, iter:7960] Loss: 60.674 | Acc: 91.826% \n",
      "[epoch:41, iter:7980] Loss: 60.901 | Acc: 91.808% \n",
      "[epoch:41, iter:8000] Loss: 61.127 | Acc: 91.763% \n",
      "[epoch:41, iter:8020] Loss: 61.432 | Acc: 91.717% \n",
      "\n",
      "Epoch: 42\n",
      "[epoch:42, iter:8056] Loss: 54.299 | Acc: 92.812% \n",
      "[epoch:42, iter:8076] Loss: 55.666 | Acc: 92.471% \n",
      "[epoch:42, iter:8096] Loss: 56.003 | Acc: 92.402% \n",
      "[epoch:42, iter:8116] Loss: 57.343 | Acc: 92.266% \n",
      "[epoch:42, iter:8136] Loss: 58.143 | Acc: 92.199% \n",
      "[epoch:42, iter:8156] Loss: 58.021 | Acc: 92.178% \n",
      "[epoch:42, iter:8176] Loss: 57.883 | Acc: 92.207% \n",
      "[epoch:42, iter:8196] Loss: 57.621 | Acc: 92.249% \n",
      "[epoch:42, iter:8216] Loss: 58.085 | Acc: 92.144% \n",
      "\n",
      "Epoch: 43\n",
      "[epoch:43, iter:8252] Loss: 50.365 | Acc: 93.418% \n",
      "[epoch:43, iter:8272] Loss: 51.088 | Acc: 93.369% \n",
      "[epoch:43, iter:8292] Loss: 52.027 | Acc: 93.262% \n",
      "[epoch:43, iter:8312] Loss: 53.988 | Acc: 93.018% \n",
      "[epoch:43, iter:8332] Loss: 55.509 | Acc: 92.758% \n",
      "[epoch:43, iter:8352] Loss: 56.103 | Acc: 92.663% \n",
      "[epoch:43, iter:8372] Loss: 56.287 | Acc: 92.603% \n",
      "[epoch:43, iter:8392] Loss: 56.399 | Acc: 92.556% \n",
      "[epoch:43, iter:8412] Loss: 56.954 | Acc: 92.446% \n",
      "\n",
      "Epoch: 44\n",
      "[epoch:44, iter:8448] Loss: 52.974 | Acc: 92.520% \n",
      "[epoch:44, iter:8468] Loss: 51.351 | Acc: 92.910% \n",
      "[epoch:44, iter:8488] Loss: 52.344 | Acc: 92.747% \n",
      "[epoch:44, iter:8508] Loss: 52.975 | Acc: 92.764% \n",
      "[epoch:44, iter:8528] Loss: 54.141 | Acc: 92.574% \n",
      "[epoch:44, iter:8548] Loss: 54.665 | Acc: 92.497% \n",
      "[epoch:44, iter:8568] Loss: 54.993 | Acc: 92.461% \n",
      "[epoch:44, iter:8588] Loss: 55.066 | Acc: 92.476% \n",
      "[epoch:44, iter:8608] Loss: 55.746 | Acc: 92.383% \n",
      "\n",
      "Epoch: 45\n",
      "[epoch:45, iter:8644] Loss: 58.799 | Acc: 92.344% \n",
      "[epoch:45, iter:8664] Loss: 56.107 | Acc: 92.500% \n",
      "[epoch:45, iter:8684] Loss: 55.252 | Acc: 92.643% \n",
      "[epoch:45, iter:8704] Loss: 55.004 | Acc: 92.637% \n",
      "[epoch:45, iter:8724] Loss: 54.510 | Acc: 92.629% \n",
      "[epoch:45, iter:8744] Loss: 54.469 | Acc: 92.653% \n",
      "[epoch:45, iter:8764] Loss: 55.232 | Acc: 92.522% \n",
      "[epoch:45, iter:8784] Loss: 55.815 | Acc: 92.446% \n",
      "[epoch:45, iter:8804] Loss: 56.179 | Acc: 92.396% \n",
      "\n",
      "Epoch: 46\n",
      "[epoch:46, iter:8840] Loss: 56.477 | Acc: 92.832% \n",
      "[epoch:46, iter:8860] Loss: 54.332 | Acc: 92.861% \n",
      "[epoch:46, iter:8880] Loss: 53.891 | Acc: 92.962% \n",
      "[epoch:46, iter:8900] Loss: 54.245 | Acc: 92.778% \n",
      "[epoch:46, iter:8920] Loss: 54.558 | Acc: 92.801% \n",
      "[epoch:46, iter:8940] Loss: 55.499 | Acc: 92.650% \n",
      "[epoch:46, iter:8960] Loss: 55.815 | Acc: 92.586% \n",
      "[epoch:46, iter:8980] Loss: 55.639 | Acc: 92.595% \n",
      "[epoch:46, iter:9000] Loss: 55.934 | Acc: 92.526% \n",
      "\n",
      "Epoch: 47\n",
      "[epoch:47, iter:9036] Loss: 54.264 | Acc: 93.027% \n",
      "[epoch:47, iter:9056] Loss: 55.241 | Acc: 92.705% \n",
      "[epoch:47, iter:9076] Loss: 55.152 | Acc: 92.702% \n",
      "[epoch:47, iter:9096] Loss: 55.252 | Acc: 92.534% \n",
      "[epoch:47, iter:9116] Loss: 53.996 | Acc: 92.703% \n",
      "[epoch:47, iter:9136] Loss: 53.640 | Acc: 92.760% \n",
      "[epoch:47, iter:9156] Loss: 54.102 | Acc: 92.656% \n",
      "[epoch:47, iter:9176] Loss: 54.595 | Acc: 92.590% \n",
      "[epoch:47, iter:9196] Loss: 55.258 | Acc: 92.556% \n",
      "\n",
      "Epoch: 48\n",
      "[epoch:48, iter:9232] Loss: 51.124 | Acc: 93.242% \n",
      "[epoch:48, iter:9252] Loss: 51.863 | Acc: 92.959% \n",
      "[epoch:48, iter:9272] Loss: 51.558 | Acc: 92.969% \n",
      "[epoch:48, iter:9292] Loss: 51.294 | Acc: 92.979% \n",
      "[epoch:48, iter:9312] Loss: 53.274 | Acc: 92.789% \n",
      "[epoch:48, iter:9332] Loss: 54.422 | Acc: 92.614% \n",
      "[epoch:48, iter:9352] Loss: 55.135 | Acc: 92.500% \n",
      "[epoch:48, iter:9372] Loss: 55.743 | Acc: 92.432% \n",
      "[epoch:48, iter:9392] Loss: 55.958 | Acc: 92.405% \n",
      "\n",
      "Epoch: 49\n",
      "[epoch:49, iter:9428] Loss: 53.030 | Acc: 93.379% \n",
      "[epoch:49, iter:9448] Loss: 51.239 | Acc: 93.457% \n",
      "[epoch:49, iter:9468] Loss: 50.407 | Acc: 93.366% \n",
      "[epoch:49, iter:9488] Loss: 49.955 | Acc: 93.335% \n",
      "[epoch:49, iter:9508] Loss: 50.737 | Acc: 93.207% \n",
      "[epoch:49, iter:9528] Loss: 51.954 | Acc: 93.005% \n",
      "[epoch:49, iter:9548] Loss: 53.176 | Acc: 92.815% \n",
      "[epoch:49, iter:9568] Loss: 53.739 | Acc: 92.773% \n",
      "[epoch:49, iter:9588] Loss: 54.155 | Acc: 92.721% \n",
      "\n",
      "Epoch: 50\n",
      "[epoch:50, iter:9624] Loss: 51.945 | Acc: 93.262% \n",
      "[epoch:50, iter:9644] Loss: 50.426 | Acc: 93.418% \n",
      "[epoch:50, iter:9664] Loss: 50.004 | Acc: 93.327% \n",
      "[epoch:50, iter:9684] Loss: 50.159 | Acc: 93.252% \n",
      "[epoch:50, iter:9704] Loss: 51.451 | Acc: 93.098% \n",
      "[epoch:50, iter:9724] Loss: 51.872 | Acc: 92.962% \n",
      "[epoch:50, iter:9744] Loss: 52.489 | Acc: 92.913% \n",
      "[epoch:50, iter:9764] Loss: 53.354 | Acc: 92.808% \n",
      "[epoch:50, iter:9784] Loss: 53.990 | Acc: 92.784% \n",
      "\n",
      "Epoch: 51\n",
      "[epoch:51, iter:9820] Loss: 51.363 | Acc: 93.516% \n",
      "[epoch:51, iter:9840] Loss: 51.174 | Acc: 93.242% \n",
      "[epoch:51, iter:9860] Loss: 52.245 | Acc: 93.086% \n",
      "[epoch:51, iter:9880] Loss: 52.319 | Acc: 92.959% \n",
      "[epoch:51, iter:9900] Loss: 52.313 | Acc: 92.891% \n",
      "[epoch:51, iter:9920] Loss: 53.053 | Acc: 92.852% \n",
      "[epoch:51, iter:9940] Loss: 53.162 | Acc: 92.832% \n",
      "[epoch:51, iter:9960] Loss: 53.932 | Acc: 92.690% \n",
      "[epoch:51, iter:9980] Loss: 54.676 | Acc: 92.543% \n",
      "\n",
      "Epoch: 52\n",
      "[epoch:52, iter:10016] Loss: 54.784 | Acc: 92.871% \n",
      "[epoch:52, iter:10036] Loss: 53.285 | Acc: 93.096% \n",
      "[epoch:52, iter:10056] Loss: 52.983 | Acc: 93.001% \n",
      "[epoch:52, iter:10076] Loss: 52.624 | Acc: 93.037% \n",
      "[epoch:52, iter:10096] Loss: 52.976 | Acc: 92.953% \n",
      "[epoch:52, iter:10116] Loss: 52.962 | Acc: 92.887% \n",
      "[epoch:52, iter:10136] Loss: 53.119 | Acc: 92.771% \n",
      "[epoch:52, iter:10156] Loss: 53.638 | Acc: 92.734% \n",
      "[epoch:52, iter:10176] Loss: 54.346 | Acc: 92.654% \n",
      "\n",
      "Epoch: 53\n",
      "[epoch:53, iter:10212] Loss: 50.180 | Acc: 93.438% \n",
      "[epoch:53, iter:10232] Loss: 50.498 | Acc: 93.340% \n",
      "[epoch:53, iter:10252] Loss: 51.551 | Acc: 93.112% \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:53, iter:10272] Loss: 53.110 | Acc: 92.861% \n",
      "[epoch:53, iter:10292] Loss: 54.070 | Acc: 92.773% \n",
      "[epoch:53, iter:10312] Loss: 54.176 | Acc: 92.715% \n",
      "[epoch:53, iter:10332] Loss: 53.440 | Acc: 92.840% \n",
      "[epoch:53, iter:10352] Loss: 53.372 | Acc: 92.810% \n",
      "[epoch:53, iter:10372] Loss: 53.634 | Acc: 92.776% \n",
      "\n",
      "Epoch: 54\n",
      "[epoch:54, iter:10408] Loss: 47.673 | Acc: 93.770% \n",
      "[epoch:54, iter:10428] Loss: 46.476 | Acc: 93.643% \n",
      "[epoch:54, iter:10448] Loss: 48.195 | Acc: 93.431% \n",
      "[epoch:54, iter:10468] Loss: 51.310 | Acc: 93.091% \n",
      "[epoch:54, iter:10488] Loss: 51.856 | Acc: 93.000% \n",
      "[epoch:54, iter:10508] Loss: 51.709 | Acc: 93.050% \n",
      "[epoch:54, iter:10528] Loss: 52.568 | Acc: 92.921% \n",
      "[epoch:54, iter:10548] Loss: 53.016 | Acc: 92.878% \n",
      "[epoch:54, iter:10568] Loss: 53.460 | Acc: 92.758% \n",
      "\n",
      "Epoch: 55\n",
      "[epoch:55, iter:10604] Loss: 49.412 | Acc: 93.379% \n",
      "[epoch:55, iter:10624] Loss: 50.128 | Acc: 93.330% \n",
      "[epoch:55, iter:10644] Loss: 49.563 | Acc: 93.359% \n",
      "[epoch:55, iter:10664] Loss: 50.531 | Acc: 93.188% \n",
      "[epoch:55, iter:10684] Loss: 51.033 | Acc: 93.191% \n",
      "[epoch:55, iter:10704] Loss: 51.558 | Acc: 93.132% \n",
      "[epoch:55, iter:10724] Loss: 52.593 | Acc: 93.008% \n",
      "[epoch:55, iter:10744] Loss: 52.701 | Acc: 92.988% \n",
      "[epoch:55, iter:10764] Loss: 52.928 | Acc: 92.941% \n",
      "\n",
      "Epoch: 56\n",
      "[epoch:56, iter:10800] Loss: 50.346 | Acc: 93.125% \n",
      "[epoch:56, iter:10820] Loss: 49.215 | Acc: 93.203% \n",
      "[epoch:56, iter:10840] Loss: 50.320 | Acc: 92.982% \n",
      "[epoch:56, iter:10860] Loss: 51.698 | Acc: 93.042% \n",
      "[epoch:56, iter:10880] Loss: 53.459 | Acc: 92.828% \n",
      "[epoch:56, iter:10900] Loss: 54.133 | Acc: 92.796% \n",
      "[epoch:56, iter:10920] Loss: 53.764 | Acc: 92.821% \n",
      "[epoch:56, iter:10940] Loss: 53.678 | Acc: 92.844% \n",
      "[epoch:56, iter:10960] Loss: 54.034 | Acc: 92.808% \n",
      "\n",
      "Epoch: 57\n",
      "[epoch:57, iter:10996] Loss: 51.121 | Acc: 93.359% \n",
      "[epoch:57, iter:11016] Loss: 49.871 | Acc: 93.564% \n",
      "[epoch:57, iter:11036] Loss: 48.018 | Acc: 93.757% \n",
      "[epoch:57, iter:11056] Loss: 50.338 | Acc: 93.433% \n",
      "[epoch:57, iter:11076] Loss: 50.528 | Acc: 93.379% \n",
      "[epoch:57, iter:11096] Loss: 51.212 | Acc: 93.245% \n",
      "[epoch:57, iter:11116] Loss: 51.794 | Acc: 93.150% \n",
      "[epoch:57, iter:11136] Loss: 52.489 | Acc: 93.081% \n",
      "[epoch:57, iter:11156] Loss: 52.539 | Acc: 93.071% \n",
      "\n",
      "Epoch: 58\n",
      "[epoch:58, iter:11192] Loss: 52.568 | Acc: 92.949% \n",
      "[epoch:58, iter:11212] Loss: 53.653 | Acc: 92.852% \n",
      "[epoch:58, iter:11232] Loss: 54.436 | Acc: 92.767% \n",
      "[epoch:58, iter:11252] Loss: 55.791 | Acc: 92.573% \n",
      "[epoch:58, iter:11272] Loss: 55.067 | Acc: 92.699% \n",
      "[epoch:58, iter:11292] Loss: 54.129 | Acc: 92.809% \n",
      "[epoch:58, iter:11312] Loss: 53.962 | Acc: 92.821% \n",
      "[epoch:58, iter:11332] Loss: 54.073 | Acc: 92.861% \n",
      "[epoch:58, iter:11352] Loss: 54.069 | Acc: 92.843% \n",
      "\n",
      "Epoch: 59\n",
      "[epoch:59, iter:11388] Loss: 54.955 | Acc: 92.676% \n",
      "[epoch:59, iter:11408] Loss: 52.699 | Acc: 92.861% \n",
      "[epoch:59, iter:11428] Loss: 51.831 | Acc: 93.060% \n",
      "[epoch:59, iter:11448] Loss: 51.314 | Acc: 93.169% \n",
      "[epoch:59, iter:11468] Loss: 50.312 | Acc: 93.320% \n",
      "[epoch:59, iter:11488] Loss: 50.375 | Acc: 93.275% \n",
      "[epoch:59, iter:11508] Loss: 51.639 | Acc: 93.128% \n",
      "[epoch:59, iter:11528] Loss: 52.556 | Acc: 92.981% \n",
      "[epoch:59, iter:11548] Loss: 52.467 | Acc: 92.962% \n",
      "\n",
      "Epoch: 60\n",
      "[epoch:60, iter:11584] Loss: 52.118 | Acc: 93.340% \n",
      "[epoch:60, iter:11604] Loss: 53.078 | Acc: 93.066% \n",
      "[epoch:60, iter:11624] Loss: 51.774 | Acc: 93.314% \n",
      "[epoch:60, iter:11644] Loss: 51.548 | Acc: 93.286% \n",
      "[epoch:60, iter:11664] Loss: 52.065 | Acc: 93.137% \n",
      "[epoch:60, iter:11684] Loss: 51.604 | Acc: 93.174% \n",
      "[epoch:60, iter:11704] Loss: 52.414 | Acc: 93.105% \n",
      "[epoch:60, iter:11724] Loss: 53.262 | Acc: 92.961% \n",
      "[epoch:60, iter:11744] Loss: 53.519 | Acc: 92.919% \n",
      "\n",
      "Epoch: 61\n",
      "[epoch:61, iter:11780] Loss: 49.151 | Acc: 93.398% \n",
      "[epoch:61, iter:11800] Loss: 48.843 | Acc: 93.486% \n",
      "[epoch:61, iter:11820] Loss: 48.907 | Acc: 93.470% \n",
      "[epoch:61, iter:11840] Loss: 49.025 | Acc: 93.428% \n",
      "[epoch:61, iter:11860] Loss: 49.905 | Acc: 93.312% \n",
      "[epoch:61, iter:11880] Loss: 50.233 | Acc: 93.278% \n",
      "[epoch:61, iter:11900] Loss: 50.442 | Acc: 93.223% \n",
      "[epoch:61, iter:11920] Loss: 50.567 | Acc: 93.230% \n",
      "[epoch:61, iter:11940] Loss: 50.821 | Acc: 93.179% \n",
      "\n",
      "Epoch: 62\n",
      "[epoch:62, iter:11976] Loss: 45.533 | Acc: 93.887% \n",
      "[epoch:62, iter:11996] Loss: 47.268 | Acc: 93.682% \n",
      "[epoch:62, iter:12016] Loss: 46.451 | Acc: 93.770% \n",
      "[epoch:62, iter:12036] Loss: 47.816 | Acc: 93.608% \n",
      "[epoch:62, iter:12056] Loss: 48.742 | Acc: 93.480% \n",
      "[epoch:62, iter:12076] Loss: 49.795 | Acc: 93.340% \n",
      "[epoch:62, iter:12096] Loss: 50.876 | Acc: 93.198% \n",
      "[epoch:62, iter:12116] Loss: 51.947 | Acc: 93.059% \n",
      "[epoch:62, iter:12136] Loss: 51.498 | Acc: 93.132% \n",
      "\n",
      "Epoch: 63\n",
      "[epoch:63, iter:12172] Loss: 52.785 | Acc: 92.578% \n",
      "[epoch:63, iter:12192] Loss: 53.143 | Acc: 92.627% \n",
      "[epoch:63, iter:12212] Loss: 53.261 | Acc: 92.611% \n",
      "[epoch:63, iter:12232] Loss: 53.306 | Acc: 92.617% \n",
      "[epoch:63, iter:12252] Loss: 53.777 | Acc: 92.645% \n",
      "[epoch:63, iter:12272] Loss: 53.224 | Acc: 92.712% \n",
      "[epoch:63, iter:12292] Loss: 53.386 | Acc: 92.695% \n",
      "[epoch:63, iter:12312] Loss: 53.044 | Acc: 92.759% \n",
      "[epoch:63, iter:12332] Loss: 53.069 | Acc: 92.771% \n",
      "\n",
      "Epoch: 64\n",
      "[epoch:64, iter:12368] Loss: 50.524 | Acc: 93.340% \n",
      "[epoch:64, iter:12388] Loss: 47.796 | Acc: 93.623% \n",
      "[epoch:64, iter:12408] Loss: 48.952 | Acc: 93.398% \n",
      "[epoch:64, iter:12428] Loss: 49.025 | Acc: 93.418% \n",
      "[epoch:64, iter:12448] Loss: 49.946 | Acc: 93.281% \n",
      "[epoch:64, iter:12468] Loss: 50.065 | Acc: 93.288% \n",
      "[epoch:64, iter:12488] Loss: 50.897 | Acc: 93.170% \n",
      "[epoch:64, iter:12508] Loss: 51.611 | Acc: 93.071% \n",
      "[epoch:64, iter:12528] Loss: 52.377 | Acc: 92.990% \n",
      "\n",
      "Epoch: 65\n",
      "[epoch:65, iter:12564] Loss: 49.428 | Acc: 93.457% \n",
      "[epoch:65, iter:12584] Loss: 47.831 | Acc: 93.652% \n",
      "[epoch:65, iter:12604] Loss: 47.625 | Acc: 93.685% \n",
      "[epoch:65, iter:12624] Loss: 48.493 | Acc: 93.540% \n",
      "[epoch:65, iter:12644] Loss: 48.519 | Acc: 93.492% \n",
      "[epoch:65, iter:12664] Loss: 48.644 | Acc: 93.545% \n",
      "[epoch:65, iter:12684] Loss: 49.340 | Acc: 93.465% \n",
      "[epoch:65, iter:12704] Loss: 49.778 | Acc: 93.401% \n",
      "[epoch:65, iter:12724] Loss: 50.090 | Acc: 93.362% \n",
      "\n",
      "Epoch: 66\n",
      "[epoch:66, iter:12760] Loss: 54.106 | Acc: 92.852% \n",
      "[epoch:66, iter:12780] Loss: 51.766 | Acc: 93.184% \n",
      "[epoch:66, iter:12800] Loss: 51.341 | Acc: 93.281% \n",
      "[epoch:66, iter:12820] Loss: 50.448 | Acc: 93.364% \n",
      "[epoch:66, iter:12840] Loss: 50.619 | Acc: 93.320% \n",
      "[epoch:66, iter:12860] Loss: 49.821 | Acc: 93.424% \n",
      "[epoch:66, iter:12880] Loss: 50.567 | Acc: 93.298% \n",
      "[epoch:66, iter:12900] Loss: 51.082 | Acc: 93.223% \n",
      "[epoch:66, iter:12920] Loss: 51.821 | Acc: 93.160% \n",
      "\n",
      "Epoch: 67\n",
      "[epoch:67, iter:12956] Loss: 49.626 | Acc: 93.359% \n",
      "[epoch:67, iter:12976] Loss: 46.192 | Acc: 93.838% \n",
      "[epoch:67, iter:12996] Loss: 46.029 | Acc: 93.815% \n",
      "[epoch:67, iter:13016] Loss: 46.410 | Acc: 93.760% \n",
      "[epoch:67, iter:13036] Loss: 47.314 | Acc: 93.617% \n",
      "[epoch:67, iter:13056] Loss: 48.932 | Acc: 93.434% \n",
      "[epoch:67, iter:13076] Loss: 49.530 | Acc: 93.351% \n",
      "[epoch:67, iter:13096] Loss: 50.120 | Acc: 93.271% \n",
      "[epoch:67, iter:13116] Loss: 51.369 | Acc: 93.112% \n",
      "\n",
      "Epoch: 68\n",
      "[epoch:68, iter:13152] Loss: 54.831 | Acc: 92.812% \n",
      "[epoch:68, iter:13172] Loss: 52.720 | Acc: 93.008% \n",
      "[epoch:68, iter:13192] Loss: 52.970 | Acc: 92.819% \n",
      "[epoch:68, iter:13212] Loss: 52.136 | Acc: 92.866% \n",
      "[epoch:68, iter:13232] Loss: 51.799 | Acc: 92.992% \n",
      "[epoch:68, iter:13252] Loss: 51.527 | Acc: 93.027% \n",
      "[epoch:68, iter:13272] Loss: 51.368 | Acc: 93.036% \n",
      "[epoch:68, iter:13292] Loss: 51.940 | Acc: 92.969% \n",
      "[epoch:68, iter:13312] Loss: 51.900 | Acc: 92.982% \n",
      "\n",
      "Epoch: 69\n",
      "[epoch:69, iter:13348] Loss: 50.384 | Acc: 93.281% \n",
      "[epoch:69, iter:13368] Loss: 50.080 | Acc: 93.369% \n",
      "[epoch:69, iter:13388] Loss: 50.795 | Acc: 93.294% \n",
      "[epoch:69, iter:13408] Loss: 49.828 | Acc: 93.315% \n",
      "[epoch:69, iter:13428] Loss: 50.073 | Acc: 93.297% \n",
      "[epoch:69, iter:13448] Loss: 50.142 | Acc: 93.275% \n",
      "[epoch:69, iter:13468] Loss: 50.360 | Acc: 93.239% \n",
      "[epoch:69, iter:13488] Loss: 50.856 | Acc: 93.171% \n",
      "[epoch:69, iter:13508] Loss: 51.411 | Acc: 93.110% \n",
      "\n",
      "Epoch: 70\n",
      "[epoch:70, iter:13544] Loss: 50.837 | Acc: 92.988% \n",
      "[epoch:70, iter:13564] Loss: 50.613 | Acc: 93.096% \n",
      "[epoch:70, iter:13584] Loss: 50.372 | Acc: 93.086% \n",
      "[epoch:70, iter:13604] Loss: 49.915 | Acc: 93.188% \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:70, iter:13624] Loss: 51.484 | Acc: 92.977% \n",
      "[epoch:70, iter:13644] Loss: 52.909 | Acc: 92.770% \n",
      "[epoch:70, iter:13664] Loss: 52.715 | Acc: 92.838% \n",
      "[epoch:70, iter:13684] Loss: 52.320 | Acc: 92.891% \n",
      "[epoch:70, iter:13704] Loss: 52.849 | Acc: 92.873% \n",
      "\n",
      "Epoch: 71\n",
      "[epoch:71, iter:13740] Loss: 49.745 | Acc: 92.988% \n",
      "[epoch:71, iter:13760] Loss: 48.847 | Acc: 93.232% \n",
      "[epoch:71, iter:13780] Loss: 48.725 | Acc: 93.451% \n",
      "[epoch:71, iter:13800] Loss: 48.900 | Acc: 93.452% \n",
      "[epoch:71, iter:13820] Loss: 49.293 | Acc: 93.438% \n",
      "[epoch:71, iter:13840] Loss: 49.144 | Acc: 93.496% \n",
      "[epoch:71, iter:13860] Loss: 49.652 | Acc: 93.401% \n",
      "[epoch:71, iter:13880] Loss: 49.150 | Acc: 93.445% \n",
      "[epoch:71, iter:13900] Loss: 49.735 | Acc: 93.333% \n",
      "\n",
      "Epoch: 72\n",
      "[epoch:72, iter:13936] Loss: 52.751 | Acc: 93.125% \n",
      "[epoch:72, iter:13956] Loss: 51.615 | Acc: 93.105% \n",
      "[epoch:72, iter:13976] Loss: 51.286 | Acc: 93.171% \n",
      "[epoch:72, iter:13996] Loss: 50.805 | Acc: 93.149% \n",
      "[epoch:72, iter:14016] Loss: 50.856 | Acc: 93.082% \n",
      "[epoch:72, iter:14036] Loss: 51.968 | Acc: 92.946% \n",
      "[epoch:72, iter:14056] Loss: 52.571 | Acc: 92.882% \n",
      "[epoch:72, iter:14076] Loss: 52.562 | Acc: 92.900% \n",
      "[epoch:72, iter:14096] Loss: 53.009 | Acc: 92.823% \n",
      "\n",
      "Epoch: 73\n",
      "[epoch:73, iter:14132] Loss: 55.639 | Acc: 92.734% \n",
      "[epoch:73, iter:14152] Loss: 48.405 | Acc: 93.672% \n",
      "[epoch:73, iter:14172] Loss: 46.646 | Acc: 93.867% \n",
      "[epoch:73, iter:14192] Loss: 46.794 | Acc: 93.765% \n",
      "[epoch:73, iter:14212] Loss: 48.159 | Acc: 93.598% \n",
      "[epoch:73, iter:14232] Loss: 48.497 | Acc: 93.477% \n",
      "[epoch:73, iter:14252] Loss: 49.366 | Acc: 93.329% \n",
      "[epoch:73, iter:14272] Loss: 50.137 | Acc: 93.298% \n",
      "[epoch:73, iter:14292] Loss: 50.322 | Acc: 93.262% \n",
      "\n",
      "Epoch: 74\n",
      "[epoch:74, iter:14328] Loss: 54.772 | Acc: 92.598% \n",
      "[epoch:74, iter:14348] Loss: 51.530 | Acc: 93.262% \n",
      "[epoch:74, iter:14368] Loss: 50.020 | Acc: 93.438% \n",
      "[epoch:74, iter:14388] Loss: 49.312 | Acc: 93.564% \n",
      "[epoch:74, iter:14408] Loss: 49.451 | Acc: 93.484% \n",
      "[epoch:74, iter:14428] Loss: 49.165 | Acc: 93.480% \n",
      "[epoch:74, iter:14448] Loss: 48.894 | Acc: 93.485% \n",
      "[epoch:74, iter:14468] Loss: 49.334 | Acc: 93.413% \n",
      "[epoch:74, iter:14488] Loss: 50.392 | Acc: 93.260% \n",
      "\n",
      "Epoch: 75\n",
      "[epoch:75, iter:14524] Loss: 44.073 | Acc: 94.355% \n",
      "[epoch:75, iter:14544] Loss: 44.541 | Acc: 94.082% \n",
      "[epoch:75, iter:14564] Loss: 45.341 | Acc: 93.978% \n",
      "[epoch:75, iter:14584] Loss: 46.104 | Acc: 93.784% \n",
      "[epoch:75, iter:14604] Loss: 46.841 | Acc: 93.707% \n",
      "[epoch:75, iter:14624] Loss: 48.008 | Acc: 93.545% \n",
      "[epoch:75, iter:14644] Loss: 48.491 | Acc: 93.460% \n",
      "[epoch:75, iter:14664] Loss: 48.619 | Acc: 93.398% \n",
      "[epoch:75, iter:14684] Loss: 48.658 | Acc: 93.420% \n",
      "\n",
      "Epoch: 76\n",
      "[epoch:76, iter:14720] Loss: 50.267 | Acc: 93.359% \n",
      "[epoch:76, iter:14740] Loss: 48.363 | Acc: 93.633% \n",
      "[epoch:76, iter:14760] Loss: 46.445 | Acc: 93.906% \n",
      "[epoch:76, iter:14780] Loss: 47.008 | Acc: 93.760% \n",
      "[epoch:76, iter:14800] Loss: 47.267 | Acc: 93.734% \n",
      "[epoch:76, iter:14820] Loss: 47.842 | Acc: 93.581% \n",
      "[epoch:76, iter:14840] Loss: 48.332 | Acc: 93.496% \n",
      "[epoch:76, iter:14860] Loss: 48.511 | Acc: 93.501% \n",
      "[epoch:76, iter:14880] Loss: 49.442 | Acc: 93.355% \n",
      "\n",
      "Epoch: 77\n",
      "[epoch:77, iter:14916] Loss: 49.576 | Acc: 92.949% \n",
      "[epoch:77, iter:14936] Loss: 50.363 | Acc: 93.105% \n",
      "[epoch:77, iter:14956] Loss: 47.938 | Acc: 93.574% \n",
      "[epoch:77, iter:14976] Loss: 48.006 | Acc: 93.511% \n",
      "[epoch:77, iter:14996] Loss: 49.158 | Acc: 93.277% \n",
      "[epoch:77, iter:15016] Loss: 49.517 | Acc: 93.311% \n",
      "[epoch:77, iter:15036] Loss: 50.222 | Acc: 93.203% \n",
      "[epoch:77, iter:15056] Loss: 50.139 | Acc: 93.232% \n",
      "[epoch:77, iter:15076] Loss: 50.143 | Acc: 93.238% \n",
      "\n",
      "Epoch: 78\n",
      "[epoch:78, iter:15112] Loss: 57.554 | Acc: 92.324% \n",
      "[epoch:78, iter:15132] Loss: 54.490 | Acc: 92.695% \n",
      "[epoch:78, iter:15152] Loss: 54.035 | Acc: 92.669% \n",
      "[epoch:78, iter:15172] Loss: 52.761 | Acc: 92.861% \n",
      "[epoch:78, iter:15192] Loss: 52.383 | Acc: 92.906% \n",
      "[epoch:78, iter:15212] Loss: 52.430 | Acc: 92.982% \n",
      "[epoch:78, iter:15232] Loss: 52.883 | Acc: 92.924% \n",
      "[epoch:78, iter:15252] Loss: 51.711 | Acc: 93.113% \n",
      "[epoch:78, iter:15272] Loss: 50.673 | Acc: 93.223% \n",
      "\n",
      "Epoch: 79\n",
      "[epoch:79, iter:15308] Loss: 49.851 | Acc: 93.496% \n",
      "[epoch:79, iter:15328] Loss: 47.466 | Acc: 93.848% \n",
      "[epoch:79, iter:15348] Loss: 47.068 | Acc: 93.835% \n",
      "[epoch:79, iter:15368] Loss: 47.850 | Acc: 93.701% \n",
      "[epoch:79, iter:15388] Loss: 48.817 | Acc: 93.602% \n",
      "[epoch:79, iter:15408] Loss: 50.098 | Acc: 93.424% \n",
      "[epoch:79, iter:15428] Loss: 50.953 | Acc: 93.237% \n",
      "[epoch:79, iter:15448] Loss: 51.059 | Acc: 93.213% \n",
      "[epoch:79, iter:15468] Loss: 51.119 | Acc: 93.210% \n",
      "\n",
      "Epoch: 80\n",
      "[epoch:80, iter:15504] Loss: 43.683 | Acc: 94.238% \n",
      "[epoch:80, iter:15524] Loss: 44.555 | Acc: 93.965% \n",
      "[epoch:80, iter:15544] Loss: 44.848 | Acc: 93.841% \n",
      "[epoch:80, iter:15564] Loss: 45.801 | Acc: 93.711% \n",
      "[epoch:80, iter:15584] Loss: 46.649 | Acc: 93.582% \n",
      "[epoch:80, iter:15604] Loss: 47.420 | Acc: 93.529% \n",
      "[epoch:80, iter:15624] Loss: 48.226 | Acc: 93.440% \n",
      "[epoch:80, iter:15644] Loss: 48.868 | Acc: 93.359% \n",
      "[epoch:80, iter:15664] Loss: 49.039 | Acc: 93.292% \n",
      "\n",
      "Epoch: 81\n",
      "[epoch:81, iter:15700] Loss: 52.016 | Acc: 93.086% \n",
      "[epoch:81, iter:15720] Loss: 48.904 | Acc: 93.389% \n",
      "[epoch:81, iter:15740] Loss: 47.082 | Acc: 93.529% \n",
      "[epoch:81, iter:15760] Loss: 46.828 | Acc: 93.608% \n",
      "[epoch:81, iter:15780] Loss: 47.857 | Acc: 93.484% \n",
      "[epoch:81, iter:15800] Loss: 48.356 | Acc: 93.402% \n",
      "[epoch:81, iter:15820] Loss: 48.659 | Acc: 93.415% \n",
      "[epoch:81, iter:15840] Loss: 49.051 | Acc: 93.381% \n",
      "[epoch:81, iter:15860] Loss: 49.578 | Acc: 93.305% \n",
      "\n",
      "Epoch: 82\n",
      "[epoch:82, iter:15896] Loss: 47.841 | Acc: 93.594% \n",
      "[epoch:82, iter:15916] Loss: 47.132 | Acc: 93.486% \n",
      "[epoch:82, iter:15936] Loss: 46.754 | Acc: 93.639% \n",
      "[epoch:82, iter:15956] Loss: 47.944 | Acc: 93.477% \n",
      "[epoch:82, iter:15976] Loss: 48.398 | Acc: 93.414% \n",
      "[epoch:82, iter:15996] Loss: 48.755 | Acc: 93.408% \n",
      "[epoch:82, iter:16016] Loss: 48.369 | Acc: 93.485% \n",
      "[epoch:82, iter:16036] Loss: 47.940 | Acc: 93.521% \n",
      "[epoch:82, iter:16056] Loss: 47.848 | Acc: 93.546% \n",
      "\n",
      "Epoch: 83\n",
      "[epoch:83, iter:16092] Loss: 47.406 | Acc: 93.164% \n",
      "[epoch:83, iter:16112] Loss: 47.506 | Acc: 93.281% \n",
      "[epoch:83, iter:16132] Loss: 47.416 | Acc: 93.451% \n",
      "[epoch:83, iter:16152] Loss: 48.123 | Acc: 93.447% \n",
      "[epoch:83, iter:16172] Loss: 48.141 | Acc: 93.375% \n",
      "[epoch:83, iter:16192] Loss: 47.931 | Acc: 93.470% \n",
      "[epoch:83, iter:16212] Loss: 48.703 | Acc: 93.312% \n",
      "[epoch:83, iter:16232] Loss: 49.042 | Acc: 93.262% \n",
      "[epoch:83, iter:16252] Loss: 49.859 | Acc: 93.142% \n",
      "\n",
      "Epoch: 84\n",
      "[epoch:84, iter:16288] Loss: 52.313 | Acc: 93.086% \n",
      "[epoch:84, iter:16308] Loss: 49.721 | Acc: 93.428% \n",
      "[epoch:84, iter:16328] Loss: 47.491 | Acc: 93.659% \n",
      "[epoch:84, iter:16348] Loss: 48.344 | Acc: 93.564% \n",
      "[epoch:84, iter:16368] Loss: 48.700 | Acc: 93.527% \n",
      "[epoch:84, iter:16388] Loss: 49.066 | Acc: 93.444% \n",
      "[epoch:84, iter:16408] Loss: 48.886 | Acc: 93.424% \n",
      "[epoch:84, iter:16428] Loss: 48.625 | Acc: 93.469% \n",
      "[epoch:84, iter:16448] Loss: 49.094 | Acc: 93.394% \n",
      "\n",
      "Epoch: 85\n",
      "[epoch:85, iter:16484] Loss: 48.917 | Acc: 93.418% \n",
      "[epoch:85, iter:16504] Loss: 46.272 | Acc: 93.789% \n",
      "[epoch:85, iter:16524] Loss: 45.674 | Acc: 93.887% \n",
      "[epoch:85, iter:16544] Loss: 46.251 | Acc: 93.838% \n",
      "[epoch:85, iter:16564] Loss: 47.330 | Acc: 93.652% \n",
      "[epoch:85, iter:16584] Loss: 48.337 | Acc: 93.525% \n",
      "[epoch:85, iter:16604] Loss: 47.831 | Acc: 93.608% \n",
      "[epoch:85, iter:16624] Loss: 47.129 | Acc: 93.652% \n",
      "[epoch:85, iter:16644] Loss: 47.588 | Acc: 93.579% \n",
      "\n",
      "Epoch: 86\n",
      "[epoch:86, iter:16680] Loss: 51.771 | Acc: 92.930% \n",
      "[epoch:86, iter:16700] Loss: 49.949 | Acc: 93.398% \n",
      "[epoch:86, iter:16720] Loss: 50.353 | Acc: 93.431% \n",
      "[epoch:86, iter:16740] Loss: 50.013 | Acc: 93.418% \n",
      "[epoch:86, iter:16760] Loss: 50.212 | Acc: 93.383% \n",
      "[epoch:86, iter:16780] Loss: 50.801 | Acc: 93.275% \n",
      "[epoch:86, iter:16800] Loss: 50.726 | Acc: 93.312% \n",
      "[epoch:86, iter:16820] Loss: 50.118 | Acc: 93.379% \n",
      "[epoch:86, iter:16840] Loss: 50.331 | Acc: 93.355% \n",
      "\n",
      "Epoch: 87\n",
      "[epoch:87, iter:16876] Loss: 46.033 | Acc: 93.691% \n",
      "[epoch:87, iter:16896] Loss: 47.156 | Acc: 93.701% \n",
      "[epoch:87, iter:16916] Loss: 48.086 | Acc: 93.535% \n",
      "[epoch:87, iter:16936] Loss: 48.907 | Acc: 93.462% \n",
      "[epoch:87, iter:16956] Loss: 48.704 | Acc: 93.488% \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:87, iter:16976] Loss: 48.961 | Acc: 93.473% \n",
      "[epoch:87, iter:16996] Loss: 49.038 | Acc: 93.424% \n",
      "[epoch:87, iter:17016] Loss: 48.954 | Acc: 93.438% \n",
      "[epoch:87, iter:17036] Loss: 49.052 | Acc: 93.448% \n",
      "\n",
      "Epoch: 88\n",
      "[epoch:88, iter:17072] Loss: 43.737 | Acc: 94.336% \n",
      "[epoch:88, iter:17092] Loss: 44.432 | Acc: 94.131% \n",
      "[epoch:88, iter:17112] Loss: 45.916 | Acc: 93.919% \n",
      "[epoch:88, iter:17132] Loss: 45.913 | Acc: 93.901% \n",
      "[epoch:88, iter:17152] Loss: 46.407 | Acc: 93.801% \n",
      "[epoch:88, iter:17172] Loss: 47.129 | Acc: 93.743% \n",
      "[epoch:88, iter:17192] Loss: 47.830 | Acc: 93.594% \n",
      "[epoch:88, iter:17212] Loss: 48.532 | Acc: 93.489% \n",
      "[epoch:88, iter:17232] Loss: 49.506 | Acc: 93.390% \n",
      "\n",
      "Epoch: 89\n",
      "[epoch:89, iter:17268] Loss: 44.521 | Acc: 94.141% \n",
      "[epoch:89, iter:17288] Loss: 44.865 | Acc: 94.043% \n",
      "[epoch:89, iter:17308] Loss: 45.604 | Acc: 93.880% \n",
      "[epoch:89, iter:17328] Loss: 45.721 | Acc: 93.779% \n",
      "[epoch:89, iter:17348] Loss: 46.468 | Acc: 93.707% \n",
      "[epoch:89, iter:17368] Loss: 47.179 | Acc: 93.675% \n",
      "[epoch:89, iter:17388] Loss: 47.354 | Acc: 93.636% \n",
      "[epoch:89, iter:17408] Loss: 47.163 | Acc: 93.665% \n",
      "[epoch:89, iter:17428] Loss: 47.427 | Acc: 93.665% \n",
      "\n",
      "Epoch: 90\n",
      "[epoch:90, iter:17464] Loss: 47.468 | Acc: 93.398% \n",
      "[epoch:90, iter:17484] Loss: 45.518 | Acc: 94.023% \n",
      "[epoch:90, iter:17504] Loss: 46.097 | Acc: 93.906% \n",
      "[epoch:90, iter:17524] Loss: 47.442 | Acc: 93.696% \n",
      "[epoch:90, iter:17544] Loss: 47.721 | Acc: 93.586% \n",
      "[epoch:90, iter:17564] Loss: 47.289 | Acc: 93.590% \n",
      "[epoch:90, iter:17584] Loss: 47.226 | Acc: 93.591% \n",
      "[epoch:90, iter:17604] Loss: 47.765 | Acc: 93.523% \n",
      "[epoch:90, iter:17624] Loss: 48.399 | Acc: 93.444% \n",
      "\n",
      "Epoch: 91\n",
      "[epoch:91, iter:17660] Loss: 48.882 | Acc: 93.477% \n",
      "[epoch:91, iter:17680] Loss: 46.618 | Acc: 93.867% \n",
      "[epoch:91, iter:17700] Loss: 46.608 | Acc: 93.822% \n",
      "[epoch:91, iter:17720] Loss: 46.352 | Acc: 93.916% \n",
      "[epoch:91, iter:17740] Loss: 46.477 | Acc: 93.902% \n",
      "[epoch:91, iter:17760] Loss: 46.752 | Acc: 93.835% \n",
      "[epoch:91, iter:17780] Loss: 46.957 | Acc: 93.789% \n",
      "[epoch:91, iter:17800] Loss: 47.518 | Acc: 93.706% \n",
      "[epoch:91, iter:17820] Loss: 47.595 | Acc: 93.700% \n",
      "\n",
      "Epoch: 92\n",
      "[epoch:92, iter:17856] Loss: 47.755 | Acc: 93.379% \n",
      "[epoch:92, iter:17876] Loss: 49.072 | Acc: 93.311% \n",
      "[epoch:92, iter:17896] Loss: 48.795 | Acc: 93.236% \n",
      "[epoch:92, iter:17916] Loss: 48.339 | Acc: 93.286% \n",
      "[epoch:92, iter:17936] Loss: 47.472 | Acc: 93.395% \n",
      "[epoch:92, iter:17956] Loss: 47.348 | Acc: 93.473% \n",
      "[epoch:92, iter:17976] Loss: 47.630 | Acc: 93.485% \n",
      "[epoch:92, iter:17996] Loss: 48.672 | Acc: 93.416% \n",
      "[epoch:92, iter:18016] Loss: 49.499 | Acc: 93.279% \n",
      "\n",
      "Epoch: 93\n",
      "[epoch:93, iter:18052] Loss: 49.822 | Acc: 93.398% \n",
      "[epoch:93, iter:18072] Loss: 47.869 | Acc: 93.516% \n",
      "[epoch:93, iter:18092] Loss: 47.608 | Acc: 93.646% \n",
      "[epoch:93, iter:18112] Loss: 47.329 | Acc: 93.652% \n",
      "[epoch:93, iter:18132] Loss: 47.731 | Acc: 93.602% \n",
      "[epoch:93, iter:18152] Loss: 47.475 | Acc: 93.669% \n",
      "[epoch:93, iter:18172] Loss: 47.542 | Acc: 93.650% \n",
      "[epoch:93, iter:18192] Loss: 48.168 | Acc: 93.523% \n",
      "[epoch:93, iter:18212] Loss: 48.441 | Acc: 93.520% \n",
      "\n",
      "Epoch: 94\n",
      "[epoch:94, iter:18248] Loss: 47.875 | Acc: 93.789% \n",
      "[epoch:94, iter:18268] Loss: 47.364 | Acc: 93.760% \n",
      "[epoch:94, iter:18288] Loss: 45.915 | Acc: 94.023% \n",
      "[epoch:94, iter:18308] Loss: 46.209 | Acc: 93.916% \n",
      "[epoch:94, iter:18328] Loss: 46.418 | Acc: 93.895% \n",
      "[epoch:94, iter:18348] Loss: 46.845 | Acc: 93.828% \n",
      "[epoch:94, iter:18368] Loss: 46.870 | Acc: 93.814% \n",
      "[epoch:94, iter:18388] Loss: 47.400 | Acc: 93.733% \n",
      "[epoch:94, iter:18408] Loss: 47.880 | Acc: 93.674% \n",
      "\n",
      "Epoch: 95\n",
      "[epoch:95, iter:18444] Loss: 45.634 | Acc: 94.219% \n",
      "[epoch:95, iter:18464] Loss: 46.252 | Acc: 93.887% \n",
      "[epoch:95, iter:18484] Loss: 46.075 | Acc: 93.906% \n",
      "[epoch:95, iter:18504] Loss: 47.032 | Acc: 93.809% \n",
      "[epoch:95, iter:18524] Loss: 47.157 | Acc: 93.750% \n",
      "[epoch:95, iter:18544] Loss: 47.193 | Acc: 93.730% \n",
      "[epoch:95, iter:18564] Loss: 47.148 | Acc: 93.739% \n",
      "[epoch:95, iter:18584] Loss: 47.302 | Acc: 93.667% \n",
      "[epoch:95, iter:18604] Loss: 47.044 | Acc: 93.678% \n",
      "\n",
      "Epoch: 96\n",
      "[epoch:96, iter:18640] Loss: 47.754 | Acc: 93.594% \n",
      "[epoch:96, iter:18660] Loss: 48.475 | Acc: 93.516% \n",
      "[epoch:96, iter:18680] Loss: 47.450 | Acc: 93.626% \n",
      "[epoch:96, iter:18700] Loss: 47.552 | Acc: 93.633% \n",
      "[epoch:96, iter:18720] Loss: 48.165 | Acc: 93.566% \n",
      "[epoch:96, iter:18740] Loss: 47.878 | Acc: 93.613% \n",
      "[epoch:96, iter:18760] Loss: 47.488 | Acc: 93.644% \n",
      "[epoch:96, iter:18780] Loss: 47.664 | Acc: 93.621% \n",
      "[epoch:96, iter:18800] Loss: 47.351 | Acc: 93.657% \n",
      "\n",
      "Epoch: 97\n",
      "[epoch:97, iter:18836] Loss: 52.884 | Acc: 92.754% \n",
      "[epoch:97, iter:18856] Loss: 51.071 | Acc: 93.018% \n",
      "[epoch:97, iter:18876] Loss: 51.230 | Acc: 93.014% \n",
      "[epoch:97, iter:18896] Loss: 50.023 | Acc: 93.218% \n",
      "[epoch:97, iter:18916] Loss: 49.808 | Acc: 93.285% \n",
      "[epoch:97, iter:18936] Loss: 49.911 | Acc: 93.307% \n",
      "[epoch:97, iter:18956] Loss: 49.769 | Acc: 93.287% \n",
      "[epoch:97, iter:18976] Loss: 49.575 | Acc: 93.337% \n",
      "[epoch:97, iter:18996] Loss: 49.453 | Acc: 93.336% \n",
      "\n",
      "Epoch: 98\n",
      "[epoch:98, iter:19032] Loss: 43.915 | Acc: 94.277% \n",
      "[epoch:98, iter:19052] Loss: 46.519 | Acc: 94.082% \n",
      "[epoch:98, iter:19072] Loss: 44.931 | Acc: 94.108% \n",
      "[epoch:98, iter:19092] Loss: 44.453 | Acc: 94.136% \n",
      "[epoch:98, iter:19112] Loss: 44.344 | Acc: 94.117% \n",
      "[epoch:98, iter:19132] Loss: 44.783 | Acc: 94.069% \n",
      "[epoch:98, iter:19152] Loss: 45.184 | Acc: 94.009% \n",
      "[epoch:98, iter:19172] Loss: 45.575 | Acc: 93.967% \n",
      "[epoch:98, iter:19192] Loss: 46.168 | Acc: 93.878% \n",
      "\n",
      "Epoch: 99\n",
      "[epoch:99, iter:19228] Loss: 48.976 | Acc: 93.555% \n",
      "[epoch:99, iter:19248] Loss: 47.624 | Acc: 93.691% \n",
      "[epoch:99, iter:19268] Loss: 46.750 | Acc: 93.750% \n",
      "[epoch:99, iter:19288] Loss: 45.613 | Acc: 93.872% \n",
      "[epoch:99, iter:19308] Loss: 46.003 | Acc: 93.855% \n",
      "[epoch:99, iter:19328] Loss: 45.581 | Acc: 93.887% \n",
      "[epoch:99, iter:19348] Loss: 46.305 | Acc: 93.806% \n",
      "[epoch:99, iter:19368] Loss: 46.582 | Acc: 93.721% \n",
      "[epoch:99, iter:19388] Loss: 46.570 | Acc: 93.709% \n",
      "\n",
      "Epoch: 100\n",
      "[epoch:100, iter:19424] Loss: 44.290 | Acc: 93.926% \n",
      "[epoch:100, iter:19444] Loss: 44.230 | Acc: 93.965% \n",
      "[epoch:100, iter:19464] Loss: 44.220 | Acc: 93.965% \n",
      "[epoch:100, iter:19484] Loss: 43.690 | Acc: 94.067% \n",
      "[epoch:100, iter:19504] Loss: 44.382 | Acc: 93.934% \n",
      "[epoch:100, iter:19524] Loss: 46.559 | Acc: 93.623% \n",
      "[epoch:100, iter:19544] Loss: 47.673 | Acc: 93.504% \n",
      "[epoch:100, iter:19564] Loss: 48.231 | Acc: 93.459% \n",
      "[epoch:100, iter:19584] Loss: 48.379 | Acc: 93.429% \n",
      "Training Finished, TotalEPOCH=100\n"
     ]
    }
   ],
   "source": [
    "# 定义是否使用GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 超参数设置\n",
    "EPOCH = 100   #遍历数据集次数\n",
    "pre_epoch = 0  # 定义已经遍历数据集的次数\n",
    "LR = 0.1        #学习率\n",
    "\n",
    "# 模型定义-ResNet\n",
    "net = ResNet18().to(device)\n",
    "\n",
    "# 定义损失函数和优化方式\n",
    "criterion = nn.CrossEntropyLoss()  #损失函数为交叉熵，多用于多分类问题\n",
    "optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4) \n",
    "#优化方式为mini-batch momentum-SGD，并采用L2正则化（权重衰减）\n",
    "\n",
    "# 训练\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Start Training, Resnet-18!\")\n",
    "    num_iters = 0\n",
    "    for epoch in range(pre_epoch, EPOCH):\n",
    "        print('\\nEpoch: %d' % (epoch + 1))\n",
    "        net.train()#因为前面用到了batch_normalization\n",
    "        sum_loss = 0.0\n",
    "        correct = 0.0\n",
    "        total = 0\n",
    "        for i, data in enumerate(trainloader, 0): \n",
    "            #用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标，\n",
    "            #下标起始位置为0，返回 enumerate(枚举) 对象。\n",
    "            \n",
    "            num_iters += 1\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()  # 清空梯度\n",
    "\n",
    "            # forward + backward\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            sum_loss += loss.item() * labels.size(0)\n",
    "            _, predicted = torch.max(outputs, 1) #选出每一列中最大的值作为预测结果\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            # 每20个batch打印一次loss和准确率\n",
    "            if (i + 1) % 20 == 0:\n",
    "                print('[epoch:%d, iter:%d] Loss: %.03f | Acc: %.3f%% '\n",
    "                        % (epoch + 1, num_iters, sum_loss / (i + 1), 100. * correct / total))\n",
    "\n",
    "    print(\"Training Finished, TotalEPOCH=%d\" % EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
