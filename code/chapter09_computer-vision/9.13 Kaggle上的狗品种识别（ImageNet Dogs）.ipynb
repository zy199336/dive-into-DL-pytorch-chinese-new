{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle上的狗品种识别（ImageNet Dogs）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本节中，我们将解决Kaggle竞赛中的犬种识别挑战，比赛的网址是https://www.kaggle.com/c/dog-breed-identification 在这项比赛中，我们尝试确定120种不同的狗。该比赛中使用的数据集实际上是著名的ImageNet数据集的子集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在本节notebook中，使用后续设置的参数在完整训练集上训练模型，大致需要40-50分钟\n",
    "# 请大家合理安排GPU时长，尽量只在训练时切换到GPU资源\n",
    "# 也可以在Kaggle上访问本节notebook：\n",
    "# https://www.kaggle.com/boyuai/boyu-d2l-dog-breed-identification-imagenet-dogs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 整理数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train和test目录下分别是训练集和测试集的图像，训练集包含10,222张图像，测试集包含10,357张图像，图像格式都是JPEG，每张图像的文件名是一个唯一的id。labels.csv包含训练集图像的标签，文件包含10,222行，每行包含两列，第一列是图像id，第二列是狗的类别。狗的类别一共有120种。\n",
    "\n",
    "我们希望对数据进行整理，方便后续的读取，我们的主要目标是：\n",
    "- 从训练集中划分出验证数据集，用于调整超参数。划分之后，数据集应该包含4个部分：划分后的训练集、划分后的验证集、完整训练集、完整测试集\n",
    "- 对于4个部分，建立4个文件夹：train, valid, train_valid, test。在上述文件夹中，对每个类别都建立一个文件夹，在其中存放属于该类别的图像。前三个部分的标签已知，所以各有120个子文件夹，而测试集的标签未知，所以仅建立一个名为unknown的子文件夹，存放所有测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../data/dog'  # 数据集目录\n",
    "label_file, train_dir, test_dir = 'labels.csv', 'train', 'test'  # data_dir中的文件夹、文件\n",
    "new_data_dir = './train_valid_test'  # 整理之后的数据存放的目录\n",
    "valid_ratio = 0.1  # 验证集所占比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_if_not_exist(path):\n",
    "    # 若目录path不存在，则创建目录\n",
    "    if not os.path.exists(os.path.join(*path)):\n",
    "        os.makedirs(os.path.join(*path))\n",
    "        \n",
    "def reorg_dog_data(data_dir, label_file, train_dir, test_dir, new_data_dir, valid_ratio):\n",
    "    # 读取训练数据标签\n",
    "    labels = pd.read_csv(os.path.join(data_dir, label_file))\n",
    "    id2label = {Id: label for Id, label in labels.values}  # (key: value): (id: label)\n",
    "\n",
    "    # 随机打乱训练数据\n",
    "    train_files = os.listdir(os.path.join(data_dir, train_dir))\n",
    "    random.shuffle(train_files)    \n",
    "\n",
    "    # 原训练集\n",
    "    valid_ds_size = int(len(train_files) * valid_ratio)  # 验证集大小\n",
    "    for i, file in enumerate(train_files):\n",
    "        img_id = file.split('.')[0]  # file是形式为id.jpg的字符串\n",
    "        img_label = id2label[img_id]\n",
    "        if i < valid_ds_size:\n",
    "            mkdir_if_not_exist([new_data_dir, 'valid', img_label])\n",
    "            shutil.copy(os.path.join(data_dir, train_dir, file),\n",
    "                        os.path.join(new_data_dir, 'valid', img_label))\n",
    "        else:\n",
    "            mkdir_if_not_exist([new_data_dir, 'train', img_label])\n",
    "            shutil.copy(os.path.join(data_dir, train_dir, file),\n",
    "                        os.path.join(new_data_dir, 'train', img_label))\n",
    "        mkdir_if_not_exist([new_data_dir, 'train_valid', img_label])\n",
    "        shutil.copy(os.path.join(data_dir, train_dir, file),\n",
    "                    os.path.join(new_data_dir, 'train_valid', img_label))\n",
    "\n",
    "    # 测试集\n",
    "    mkdir_if_not_exist([new_data_dir, 'test', 'unknown'])\n",
    "    for test_file in os.listdir(os.path.join(data_dir, test_dir)):\n",
    "        shutil.copy(os.path.join(data_dir, test_dir, test_file),\n",
    "                    os.path.join(new_data_dir, 'test', 'unknown'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reorg_dog_data(data_dir, label_file, train_dir, test_dir, new_data_dir, valid_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图像增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    # 随机对图像裁剪出面积为原图像面积0.08~1倍、且高和宽之比在3/4~4/3的图像，再放缩为高和宽均为224像素的新图像\n",
    "    transforms.RandomResizedCrop(224, scale=(0.08, 1.0),  \n",
    "                                 ratio=(3.0/4.0, 4.0/3.0)),\n",
    "    # 以0.5的概率随机水平翻转\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # 随机更改亮度、对比度和饱和度\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "    transforms.ToTensor(),\n",
    "    # 对各个通道做标准化，(0.485, 0.456, 0.406)和(0.229, 0.224, 0.225)是在ImageNet上计算得的各通道均值与方差\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # ImageNet上的均值和方差\n",
    "])\n",
    "\n",
    "# 在测试集上的图像增强只做确定性的操作\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    # 将图像中央的高和宽均为224的正方形区域裁剪出来\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_data_dir目录下有train, valid, train_valid, test四个目录\n",
    "# 这四个目录中，每个子目录表示一种类别，目录中是属于该类别的所有图像\n",
    "train_ds = torchvision.datasets.ImageFolder(root=os.path.join(new_data_dir, 'train'),\n",
    "                                            transform=transform_train)\n",
    "valid_ds = torchvision.datasets.ImageFolder(root=os.path.join(new_data_dir, 'valid'),\n",
    "                                            transform=transform_test)\n",
    "train_valid_ds = torchvision.datasets.ImageFolder(root=os.path.join(new_data_dir, 'train_valid'),\n",
    "                                            transform=transform_train)\n",
    "test_ds = torchvision.datasets.ImageFolder(root=os.path.join(new_data_dir, 'test'),\n",
    "                                            transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_iter = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_iter = torch.utils.data.DataLoader(valid_ds, batch_size=batch_size, shuffle=True)\n",
    "train_valid_iter = torch.utils.data.DataLoader(train_valid_ds, batch_size=batch_size, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=False)  # shuffle=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net(device):\n",
    "    finetune_net = models.resnet34(pretrained=True)  # 预训练的resnet34网络\n",
    "    #finetune_net.load_state_dict(torch.load('/home/kesci/input/resnet347742/resnet34-333f7ec4.pth'))\n",
    "    for param in finetune_net.parameters():  # 冻结参数\n",
    "        param.requires_grad = False\n",
    "    # 原finetune_net.fc是一个输入单元数为512，输出单元数为1000的全连接层\n",
    "    # 替换掉原finetune_net.fc，新finetuen_net.fc中的模型参数会记录梯度\n",
    "    finetune_net.fc = nn.Sequential(\n",
    "        nn.Linear(in_features=512, out_features=256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=256, out_features=120)  # 120是输出类别数\n",
    "    )\n",
    "    return finetune_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss_acc(data_iter, net, device):\n",
    "    # 计算data_iter上的平均损失与准确率\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    is_training = net.training  # Bool net是否处于train模式\n",
    "    net.eval()\n",
    "    l_sum, acc_sum, n = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l_sum += l.item() * y.shape[0]\n",
    "            acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "    net.train(is_training)  # 恢复net的train/eval状态\n",
    "    return l_sum / n, acc_sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, valid_iter, num_epochs, lr, wd, device, lr_period,\n",
    "          lr_decay):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.fc.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
    "    net = net.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, n, start = 0.0, 0, time.time()\n",
    "        if epoch > 0 and epoch % lr_period == 0:  # 每lr_period个epoch，学习率衰减一次\n",
    "            lr = lr * lr_decay\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        for X, y in train_iter:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.item() * y.shape[0]\n",
    "            n += y.shape[0]\n",
    "        time_s = \"time %.2f sec\" % (time.time() - start)\n",
    "        if valid_iter is not None:\n",
    "            valid_loss, valid_acc = evaluate_loss_acc(valid_iter, net, device)\n",
    "            epoch_s = (\"epoch %d, train loss %f, valid loss %f, valid acc %f, \"\n",
    "                       % (epoch + 1, train_l_sum / n, valid_loss, valid_acc))\n",
    "        else:\n",
    "            epoch_s = (\"epoch %d, train loss %f, \"\n",
    "                       % (epoch + 1, train_l_sum / n))\n",
    "        print(epoch_s + time_s + ', lr ' + str(lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调参"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 不停调整参数并在训练/验证集上验证,当取得较好参数时, 再在完整数据集中进行训练,并最后进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, lr_period, lr_decay = 20, 10, 0.1\n",
    "lr, wd = 0.03, 1e-4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /home/wrjs/.cache/torch/checkpoints/resnet34-333f7ec4.pth\n",
      "100%|██████████| 83.3M/83.3M [00:27<00:00, 3.22MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train loss 3.421148, valid loss 1.134548, valid acc 0.679061, time 101.09 sec, lr 0.03\n",
      "epoch 2, train loss 1.520380, valid loss 0.762263, valid acc 0.768102, time 98.36 sec, lr 0.03\n",
      "epoch 3, train loss 1.294325, valid loss 0.680419, valid acc 0.775930, time 100.51 sec, lr 0.03\n",
      "epoch 4, train loss 1.221176, valid loss 0.597771, valid acc 0.804305, time 94.87 sec, lr 0.03\n",
      "epoch 5, train loss 1.156143, valid loss 0.589016, valid acc 0.808219, time 87.20 sec, lr 0.03\n",
      "epoch 6, train loss 1.126487, valid loss 0.594095, valid acc 0.802348, time 95.84 sec, lr 0.03\n",
      "epoch 7, train loss 1.110660, valid loss 0.574036, valid acc 0.807241, time 93.26 sec, lr 0.03\n",
      "epoch 8, train loss 1.074657, valid loss 0.571067, valid acc 0.811155, time 91.42 sec, lr 0.03\n",
      "epoch 9, train loss 1.098592, valid loss 0.569241, valid acc 0.806262, time 96.78 sec, lr 0.03\n",
      "epoch 10, train loss 1.058438, valid loss 0.551097, valid acc 0.819961, time 91.21 sec, lr 0.03\n",
      "epoch 11, train loss 0.931058, valid loss 0.476972, valid acc 0.838552, time 90.36 sec, lr 0.003\n",
      "epoch 12, train loss 0.894830, valid loss 0.464044, valid acc 0.842466, time 92.48 sec, lr 0.003\n",
      "epoch 13, train loss 0.870960, valid loss 0.462843, valid acc 0.837573, time 83.84 sec, lr 0.003\n",
      "epoch 14, train loss 0.884814, valid loss 0.452604, valid acc 0.846380, time 91.93 sec, lr 0.003\n",
      "epoch 15, train loss 0.881059, valid loss 0.447879, valid acc 0.850294, time 89.57 sec, lr 0.003\n",
      "epoch 16, train loss 0.870742, valid loss 0.458355, valid acc 0.850294, time 90.27 sec, lr 0.003\n",
      "epoch 17, train loss 0.864220, valid loss 0.452857, valid acc 0.844423, time 92.76 sec, lr 0.003\n",
      "epoch 18, train loss 0.882113, valid loss 0.460087, valid acc 0.847358, time 88.93 sec, lr 0.003\n",
      "epoch 19, train loss 0.858172, valid loss 0.452508, valid acc 0.847358, time 94.64 sec, lr 0.003\n",
      "epoch 20, train loss 0.864377, valid loss 0.451766, valid acc 0.845401, time 82.83 sec, lr 0.003\n"
     ]
    }
   ],
   "source": [
    "net = get_net(device)\n",
    "train(net, train_iter, valid_iter, num_epochs, lr, wd, device, lr_period, lr_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完整数据集上训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train loss 3.277695, time 102.65 sec, lr 0.03\n",
      "epoch 2, train loss 1.464980, time 100.90 sec, lr 0.03\n",
      "epoch 3, train loss 1.280180, time 103.68 sec, lr 0.03\n",
      "epoch 4, train loss 1.229663, time 105.13 sec, lr 0.03\n",
      "epoch 5, train loss 1.189382, time 104.47 sec, lr 0.03\n",
      "epoch 6, train loss 1.146835, time 100.94 sec, lr 0.03\n",
      "epoch 7, train loss 1.131901, time 103.73 sec, lr 0.03\n",
      "epoch 8, train loss 1.074839, time 96.77 sec, lr 0.03\n",
      "epoch 9, train loss 1.080261, time 102.05 sec, lr 0.03\n",
      "epoch 10, train loss 1.026582, time 100.58 sec, lr 0.03\n",
      "epoch 11, train loss 0.944581, time 100.63 sec, lr 0.003\n",
      "epoch 12, train loss 0.892754, time 105.81 sec, lr 0.003\n",
      "epoch 13, train loss 0.884726, time 100.50 sec, lr 0.003\n",
      "epoch 14, train loss 0.865320, time 99.56 sec, lr 0.003\n",
      "epoch 15, train loss 0.859324, time 103.59 sec, lr 0.003\n",
      "epoch 16, train loss 0.885917, time 101.66 sec, lr 0.003\n",
      "epoch 17, train loss 0.868039, time 100.02 sec, lr 0.003\n",
      "epoch 18, train loss 0.881039, time 101.85 sec, lr 0.003\n",
      "epoch 19, train loss 0.866405, time 99.71 sec, lr 0.003\n",
      "epoch 20, train loss 0.882724, time 102.05 sec, lr 0.003\n"
     ]
    }
   ],
   "source": [
    "net = get_net(device)\n",
    "train(net, train_valid_iter, None, num_epochs, lr, wd, device, lr_period, lr_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提交结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for X, _ in test_iter:\n",
    "    X = X.to(device)\n",
    "    output = net(X)\n",
    "    output = torch.softmax(output, dim=1)\n",
    "    preds += output.tolist()\n",
    "ids = sorted(os.listdir(os.path.join(new_data_dir, 'test/unknown')))\n",
    "with open('submission.csv', 'w') as f:\n",
    "    f.write('id,' + ','.join(train_valid_ds.classes) + '\\n')\n",
    "    for i, output in zip(ids, preds):\n",
    "        f.write(i.split('.')[0] + ',' + ','.join(\n",
    "            [str(num) for num in output]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
